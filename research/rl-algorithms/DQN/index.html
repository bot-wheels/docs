
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../../assets/3d-car.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.49">
    
    
      
        <title>Deep Q Network - Bot-Wheels</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.6f8fc17f.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#deep-q-network" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Bot-Wheels" class="md-header__button md-logo" aria-label="Bot-Wheels" data-md-component="logo">
      
  <img src="../../../assets/3d-car.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Bot-Wheels
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Deep Q Network
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/bot-wheels" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    bot-wheels
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Bot-Wheels" class="md-nav__button md-logo" aria-label="Bot-Wheels" data-md-component="logo">
      
  <img src="../../../assets/3d-car.png" alt="logo">

    </a>
    Bot-Wheels
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/bot-wheels" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    bot-wheels
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Project Guide
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Project Guide
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../project-guide/working-with-project/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Working With Project
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../project-guide/poetry/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Poetry Usage Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../project-guide/codebase-standardization-guidelines/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Codebase Standardization Guidelines
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../project-guide/rocket-learn-setup/rocket-learn-setup/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Rocket Learn Setup
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../project-guide/rlgym_ppo/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Rl-Gym-PPO Setup
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../project-guide/google-drive-manager/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Google Drive Manager Tutorial
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../project-guide/Running-Bot-Training-with-PPO/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Bot Training with PPO
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Research
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Research
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../interface-overlay-research.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Interface Overlay Research
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../playing-in-online-games/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Playing in Online Games
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../rocket-league-bot-setup/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Rocket League Bot Setup
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../implement-movement-testing-script-for-bot-actions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Implement Movement Testing Script for Bot Actions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../match-against-ai-bots/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Match Against AI Bots
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../research-and-documentation-of-algorithm-categories-for-rocket-league-bot/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Research and Documentation of Algorithm Categories for Rocket League Bot
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../research-about-different-reward-functions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Research about different reward functions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_8" >
        
          
          <label class="md-nav__link" for="__nav_2_8" id="__nav_2_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    RL algorithms
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_8">
            <span class="md-nav__icon md-icon"></span>
            RL algorithms
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../A3C/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    A3C
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../model-based-algorithms-for-rocket-league-bots-alphazero/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model based algorithms for RL bots alphazero
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../police-optimization-documentation-summer/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Police Optimization Documentation from the summer
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Biweekly Reports
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Biweekly Reports
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../biweekly-reports/weekly-report-1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week 1-2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../biweekly-reports/weekly-report-2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week 3-4
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../biweekly-reports/weekly-report-3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week 5-6
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../biweekly-reports/weekly-report-4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week 7-8
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../biweekly-reports/weekly-report-5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week 9-11
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../biweekly-reports/summary-report/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Summary
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="deep-q-network">Deep Q Network</h1>
<p><strong><em>Theoretical Background:</em></strong></p>
<p>Q-learning is a basic reinforcement learning algorithm. It is model-free (meaning it doesn‚Äôt need a model of the environment), off-policy (it learns regardless of the strategy we follow), and value-based. Q-learning uses a Q-table to store and update values for each state-action pair. These values are updated using the Bellman equation. What‚Äôs interesting is that the Bellman equation doesn‚Äôt rely on the previous state but instead uses the current state and the estimated value of the next state.</p>
<p><img alt="image1" src="../../../assets/DQN/image1.png" /></p>
<p><em>Image created by:Mike Wang</em></p>
<p>A major issue with Q-learning is that the Q-table method becomes impractical when there are too many states and actions to calculate. In such cases, the table can grow too large.</p>
<p>The solution to this problem is Deep Q-learning (DQL), which is an extension of Q-learning that uses a neural network instead of a Q-table. The difference is that in Q-learning, the Q-table gives us simple values, but in DQL, the neural network outputs action-value pairs.</p>
<p><img alt="image2" src="../../../assets/DQN/image2.png" /></p>
<p><em>Image created by:Sergios Karagiannakos</em></p>
<p>Deep Q-Network (DQN) refers to the neural network used in DQL to approximate Q-values. This helps in handling environments with large or continuous state spaces, making decision-making more efficient.</p>
<p><strong><em>Implementation:</em></strong><br />
Here you can read about the main steps to create basic DQN.</p>
<p><strong>DQN Step-by-Step algorithm</strong>  </p>
<ol>
<li><strong>Create Policy Neural Network:</strong><br />
   Start by defining the policy network, which is a neural network responsible for approximating the Q-values for a given state-action pair. The network takes the current state as input and outputs a Q-value for each possible action in that state.  </li>
<li><strong>Create Target Network:</strong><br />
   Copy the policy network to create the target network.  </li>
<li>This is a copy of the policy network that is updated periodically to help stabilize the learning process. By using a fixed target network, we reduce the risk of overestimating Q-values, which can occur when both the predicted and target Q-values are updated simultaneously.  </li>
<li>The target network is synchronized with the policy network after a fixed number of steps.  </li>
<li><strong>Action Selection with Epsilon-Greedy Exploration:</strong><br />
   During training, the agent chooses actions using an epsilon-greedy strategy:  </li>
<li>With probability epsilon (ùúñ), the agent selects a random action.  </li>
<li>With probability (1 ‚Äì epsilon), the agent selects the action with the highest Q-value predicted by the policy network.  </li>
<li>As training progresses, epsilon is gradually reduced to encourage more exploitation over time.  </li>
<li><strong>Store Experiences in Replay Buffer:</strong><br />
   After every action, store the experience in a replay buffer (also called experience replay).  </li>
<li>Each experience consists of a tuple (state, action, reward, next state, done), representing a single step from one state to another as a result of taking a specific action.  </li>
<li>Replay Memory is a buffer that allows the agent to store and randomly sample mini-batches of experiences to break the correlation between consecutive transitions. It helps improve the stability and convergence of the learning process.  </li>
<li><strong>Put Information into the Policy Network:</strong><br />
   Sample a mini-batch of experiences from the replay buffer and input the current state (s) into the policy network. The policy network will output the predicted Q-values for all actions in that state.  </li>
<li><strong>Obtain Q-value from Target Network:</strong><br />
   For each experience in the mini-batch, pass the next state (s') through the target network to get the Q-value for the next state. Use these Q-values to compute the target Q-value.  </li>
<li><strong>Calculate the Target Q-value using the Bellman Equation</strong><br />
   The target Q-value (ideal reward) is calculated using the Bellman equation:</li>
</ol>
<p>$$
   Q_{\text{target}}(s, a) = r + \gamma \max_{a'} Q_{\text{target}}(s', a')
   $$</p>
<p>where:</p>
<ul>
<li>( r ) is the reward received after taking action ( a ) in state ( s ),</li>
<li>( \gamma ) is the discount factor that accounts for future rewards,</li>
<li>
<p>( \max_{a'} Q_{\text{target}}(s', a') ) is the maximum Q-value for the next state ( s' ) as predicted by the target network.</p>
</li>
<li>
<p><strong>Compute the Loss and Backpropagate</strong><br />
   The loss is calculated as the mean squared error between the predicted Q-values from the policy network and the target Q-values from the target network:</p>
</li>
</ul>
<p>$$
   \text{Loss} = \frac{1}{N} \sum_{i=1}^{N} \left( Q_{\text{predicted}}(s_i, a_i) - Q_{\text{target}}(s_i, a_i) \right)^2
   $$</p>
<ul>
<li>
<p>Use this loss to perform backpropagation and update the weights of the policy network using stochastic gradient descent.</p>
</li>
<li>
<p><strong>Train the Policy Network:</strong><br />
   The policy network is updated to minimize the difference between the predicted Q-values and the target Q-values, ensuring that it learns to make better action-value predictions over time.  </p>
</li>
<li><strong>Repeat Steps 3-8:</strong><br />
    Continue the process by repeating steps 3 through 8:  <ul>
<li>Select actions using epsilon-greedy.  </li>
<li>Store experiences in the replay buffer.  </li>
<li>Sample mini-batches, calculate the loss, and update the policy network.  </li>
</ul>
</li>
<li><strong>Synchronize Policy and Target Networks:</strong><br />
    After a fixed number of steps (usually after several episodes or steps), synchronize the target network with the policy network by copying the weights from the policy network to the target network. This keeps the target network up to date, but prevents it from changing too frequently.</li>
</ul>
<p><strong>Recommend to watch this video:</strong><br />
<a href="https://youtu.be/EUrWGTCGzlA?si=67Bblssy8OYbeTwJ"><strong>https://youtu.be/EUrWGTCGzlA?si=67Bblssy8OYbeTwJ</strong></a></p>
<p><strong>Exemples of impementation:</strong><br />
<a href="https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py"><strong>cleanrl/cleanrl/dqn_atari.py at master ¬∑ vwxyzjn/cleanrl (github.com)</strong></a><br />
<a href="https://github.com/JayBaileyCS/RLAlgorithms/blob/main/DQN_From_Scratch_Atari.ipynb"><strong>RLAlgorithms/DQN_From_Scratch_Atari.ipynb at main ¬∑ JayBaileyCS/RLAlgorithms (github.com)</strong></a></p>
<p><strong><em>Problems and solutions:</em></strong></p>
<p>A common problem when using neural networks in reinforcement learning is maintaining stability during the training process. To address this issue, we can use <strong>Double DQN</strong>. This approach involves using two DQNs: one is a copy of the network, which is used to generate the predicted term, and the second one is the actual DQN used for training, responsible for generating the target term. This method helps to stabilize training and improves the accuracy of the Q-values, particularly during the early stages.</p>
<p>Another common challenge is tuning the correct hyperparameters for the neural network. Some key parameters include:</p>
<ul>
<li><strong>Epsilon value</strong>: It's better to start with a smaller epsilon to prevent the model from excessive exploration before the exploitation phase begins.  </li>
<li><strong>Learning rate</strong>: It should not be too large in order to slow down the learning process, which helps in stabilizing the training.  </li>
<li><strong>Memory size</strong>: The replay memory should be large enough to store sufficient experiences for effective learning.</li>
</ul>
<p><strong>Here you can read how different parameters can be involved in results:</strong><br />
<a href="https://drawar.github.io/blog/2019/05/12/lunar-lander-dqn.html"><strong>Solving Lunar Lander with Double Dueling Deep Q-Network and PyTorch (drawar.github.io)</strong></a></p>
<p><strong><em>Resources:</em></strong></p>
<p>First time using:<br />
<a href="https://arxiv.org/pdf/1312.5602">1312.5602 (arxiv.org)</a><br />
<a href="https://arxiv.org/pdf/1711.07478">1711.07478 (arxiv.org)</a></p>
<p>Foundation about RL:<br />
<a href="https://towardsdatascience.com/the-secrets-behind-reinforcement-learning-25b87befb2d3">The secrets behind Reinforcement Learning | by Sergios Karagiannakos | Towards Data Science</a></p>
<p>Foundation about Q-learning:<br />
<a href="https://towardsdatascience.com/q-learning-and-deep-q-networks-436380e8396a">Q Learning and Deep Q Networks. The journey to Reinforcement learning‚Ä¶ | by Sergios Karagiannakos | Towards Data Science</a></p>
<p>Compera to Policy Gradient:<br />
<a href="https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html">Deep Q Network vs Policy Gradients - An Experiment on VizDoom with Keras | Felix Yu (flyyufelix.github.io)</a></p>
<p>Useful repo:<br />
<a href="https://github.com/curefate/RLgym/blob/master/DQN/DQNtest.py">RLgym/DQN/DQNtest.py at master ¬∑ curefate/RLgym (github.com)</a><br />
<a href="https://github.com/lexiconium/RL-Gym-PyTorch/tree/main/CartPole/DQN">RL-Gym-PyTorch/CartPole/DQN at main ¬∑ lexiconium/RL-Gym-PyTorch (github.com)</a><br />
<a href="https://github.com/udacity/deep-learning/blob/master/reinforcement/Q-learning-cart.ipynb"><strong>deep-learning/reinforcement/Q-learning-cart.ipynb at master ¬∑ udacity/deep-learning (github.com)</strong></a>  </p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/bot-wheels" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.88dd0f4e.min.js"></script>
      
    
  </body>
</html>