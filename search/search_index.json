{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Bot Wheels Documentation","text":""},{"location":"#projects-purpose","title":"Project's purpose","text":"<p>The goal of the project is to create a bot for the game Rocket League, which will compete at an assumed average player level.</p> <ul> <li>The bot will use the PPO (Proximal Policy Optimization) algorithm for learning, with a possible switch to alternatively DQN or A3C.</li> <li>The main goal is to achieve &gt;50% wins in 20 matches against Silver level players (locally or if possible in online play)/Bot Psyonix Rookie (if successful Psyonix Allstar).</li> <li>The bot will be optimized to run at 30 FPS on hardware with i9-14900HX and RTX 4080.</li> <li>Create visualization of the bot's states.</li> </ul>"},{"location":"#example-of-a-rocket-league-bot","title":"Example of a rocket league bot","text":""},{"location":"#work-plan","title":"Work plan","text":"Week Scope of Work Week 1 - Setting up a repository on GitHub; Environment configuration (Python, TensorFlow, RLGym) - Analysis of Rocket League game mechanics Week 2 - Defining key tasks for the bot - Initial implementation of the PPO algorithm Week 3 - Implementation of basic bot functions (movement, ball interaction) - Introducing simple game logs (console logs) Milestone 1 - Progress report on project assumptions and preliminary results Week 4 - Implementing the PPO algorithm \u2013 agent training - Testing different approaches (e.g. different rewards, parameters) Week 5 - PPO training and optimization (testing in more games) - Results analysis Week 6 - Further PPO optimization - Analyzing bot efficiency in different game scenarios (e.g. 1v1 local, bots) Milestone 2 - Report on PPO test results - Bot performance evaluation Week 7 - Fine-tuning the bot (adjusting strategy, changing parameters in the algorithm Week 8 - Implementation of a system for visualizing bot states and actions, along with fine-tuning Week 9 - Bug fixes and tests against various opponents - Introducing logs to monitor bot actions Week 10 - Final fixes - Project documentation; preparation of the final report and presentation Deadline - Final bot presentation and report"},{"location":"#bot-wheels-github","title":"Bot-Wheels Github","text":"<ul> <li>bot-wheels-core</li> <li>docs</li> </ul>"},{"location":"#team","title":"Team","text":"<ul> <li>Ivanytska Anna</li> <li>Jakub Ciso\u0144</li> <li>Kacper Drozdowski</li> <li>Konrad Siemi\u0105tkowski</li> <li>Dawid Mielewczyk</li> <li>Mateusz Go\u015bciniecki</li> <li>Max Nadir</li> <li>Micha\u0142 Pryba</li> <li>Micha\u0142 Zarzycki</li> <li>Igor Malkovsky</li> <li>Wojtek Szamocki</li> </ul>"},{"location":"#used-tools","title":"Used tools","text":"<ul> <li>Python 3.9.13 - Used programming language</li> <li>RLGym - Open-sourced Python API for Reinforcement Learning Environments</li> <li>RLBot - Create and share Rocket League bots for offline play</li> <li>Ruff - Linter</li> <li>rocket-learn/Redis - attempt to implement</li> </ul>"},{"location":"#summary-of-the-project","title":"Summary of the Project","text":"<p>The implementation of PPO and A2C algorithms was completed. However, the absence of a functional test environment hindered the ability to verify the bot's performance against the target win rate of 50%. Despite this, the bot achieved a stable runtime of over 30 FPS, meeting a critical technical requirement. Visualizations were successfully delivered as planned. A significant issue arose due to the release of a new version of a key library just days before the project deadline. This incompatibility with the existing setup resulted in the project's failure to operate as intended. Team performance varied significantly, with delays in task completion and insufficient communication of progress from some members. These factors, coupled with over-reliance on individual accountability, contributed to the project's shortcomings. Out of the three planned algorithm implementations, two were completed, but none of the trained models met the target objectives due to insufficient training time. The project failed to achieve its primary objectives. Limited contributions from certain team members, especially near the project's conclusion, had a severe impact. The team's dynamics revealed a division between committed contributors and those who disengaged. The lack of full engagement prevented the team from meeting the intended goals, despite partial progress in algorithm development and achieving stable bot performance. Estimated completion of approximately 65% of the intended goal.</p>"},{"location":"biweekly-reports/summary-report/","title":"Subject: Project Summary","text":"<p>Project Title: BotWheels</p> <p>Report Number: 6</p> <p>Date: January 8, 2025</p>"},{"location":"biweekly-reports/summary-report/#1-progress-overview","title":"1. Progress Overview","text":""},{"location":"biweekly-reports/summary-report/#project-objectives","title":"Project Objectives","text":"<p>The goal of this project is to develop a Rocket League bot that competes at the skill level of an average player. The bot primarily uses the Proximal Policy Optimization (PPO) algorithm for learning, with the possibility of transitioning to alternative algorithms.</p> <p>The main objectives are:</p> <ul> <li> <p>Implementation of Reinforcement Learning Algorithm   PPO, DQN, and A3C algorithm implementations are underway, with a focus on PPO for training.</p> </li> <li> <p>Achieve a win rate of over 50% in 20 matches   The bot is being tested against both local silver-level players and Psyonix bots, with the potential to compete against Allstar-level bots if performance allows.</p> </li> <li> <p>Optimize the bot's performance   The bot is being optimized to run at 30 FPS on a system with an Intel i9-14900HX CPU and RTX 4080 GPU.</p> </li> <li> <p>Develop visualizations   Visualizations of the bot's states and actions are being developed to improve decision-making and performance analysis.</p> </li> </ul> <p>The approaches based on A2C and PPO algorithms were implemented, but the lack of a test environment prevented full testing. As a result, it is not possible to conclude whether the bots would have achieved the target of 50% win rate. On the positive side, the bots run stably at over 30 FPS, which meets the objective. Visualizations were delivered as planned.</p>"},{"location":"biweekly-reports/summary-report/#2-team-member-contributions-previous-grades-and-proposed-final-grade","title":"2. Team Member Contributions, Previous Grades, and Proposed Final Grade","text":"Team Member Previous Grades Average from Previous Periods Overall Grade Summary of Contributions Proposed Final Grade Jakub Ciso\u0144 4.5, 4.5, 4, 4.5, 4 4.3 5 Project management, conducting weekly status meetings, task distribution, project oversight, work organization, reporting, and preparing presentations and documentation. The downside was limited involvement in coding, though the focus was on management tasks while maintaining adequate availability and delivering documents on time. 4.5 Kacper Drozdowski 3.5, 3.5, 3, 4, 4 3.6 3.5 Work on implementing the A2C algorithm, specifically creating the reward module, which played a key role in training. From early December, assigned the task of training the bot using this algorithm, but late start led to a bot with minimal functionality (moving but stuck in loops). Overall, relatively little code was written, and communication issues were noted. 3.5 Mateusz Go\u015bciniecki 4, 5, 4, 4, 3 4.0 4 Key implementations include attempting to integrate Rocket-Learn, an alternative library, but this was not successfully implemented. Assigned the task of training with PPO in early December, but delayed start led to unsatisfactory results. 4 Anna Ivanytska 2, 3.5, 3.5, 3, 3 3.0 3.5 Assisted with bot's diagnostic visualizations in gameplay, including the creation of an event logger for gameplay events. Explored DQN, but late start resulted in the abandonment of its implementation. Poor communication, with multiple reminders required for task initiation. Attempted documentation creation but failed to integrate it properly into the main branch. 3 Igor Malkovskiy 4.5, 4, 2, 4, 3 3.5 2 Initially engaged well, but later reduced involvement with frequent absences from meetings and communication issues. Assigned to train the bot using PPO but failed to deliver any results, with a late start and poor follow-through. 3 Dawid Mielewczyk 5, 5, 4.5, 4.5, 4 4.6 5 Co-leader, primarily responsible for managing the code, including support, code reviews, ensuring correct coding practices, and integrating Discord bots and linters. Assigned the task of creating a simple CLI for switching between gameplay and training with different algorithms. However, due to issues with task deliveries by other team members, the task was simplified. 5 Camille Nadir 3, 3, 2, 3.5, 3 2.9 2 Large discrepancy between declared tasks and actual code delivery. Failure to complete tasks significantly impacted project results. Informed about writing code but kept it private and refused to push it to the repository. 2 Micha\u0142 Pryba 5, 5, 5, 5, 5 5.0 5 Key contributor, with primary responsibility for A2C algorithm implementation and excellent communication. Took initiative in the development process and later shifted to support role to assist with workload reduction and provide assistance to other team members. 5 Wojciech Szamocki 5, 5, 5, 4.5, 4.5 4.8 5 Main developer for diagnostic visualizations, effective communication, no need for reminders. Also shifted to support role as his general engagement increased. 5 Konrad Siemi\u0105tkowski 2, 4, 4, 4, 3.5 3.5 3.5 Worked on A2C implementation, documentation, and metrics collection. Limited contribution to the actual project, assigned task of training with A2C, but no satisfactory results due to the late start. 3.5 Micha\u0142 Zarzycki 3.5, 5, 4.5, 4.5, 3.5 4.2 4.5 Good start with involvement in setting up PPO algorithm foundations. Reduced contribution around project deadline, affecting the flow of information and project progress. 4"},{"location":"biweekly-reports/summary-report/#3-challenges-and-problems-encountered","title":"3. Challenges and Problems Encountered","text":"<p>A new version of a key library was released just a few days before the project deadline, which caused significant issues as the current setup could not run with the new version. Consequently, the project is not functioning as expected at this time.  </p> <p>Overall, the project was hindered by delays in task completion by individual team members. A major contributing factor was a lack of communication regarding problems, despite repeated requests to update progress. A key mistake in management was placing too much trust in human factors.</p> <p>Out of the three planned algorithm implementations, two were completed, but none of the trained models met the target objectives due to insufficient training hours. Further details on the implementation can be found in the documentation.</p>"},{"location":"biweekly-reports/summary-report/#4-summary","title":"4. Summary","text":"<p>The project did not meet the planned objectives. The lack of contribution from certain team members, particularly toward the end, and the failure to deliver key metrics hampered progress. The team dynamics clearly divided into two groups: those who were committed and engaged with the project, and those who abandoned it close to the deadline. The latter group severely impacted the project's success.  </p>"},{"location":"biweekly-reports/weekly-report-1/","title":"BoT Project Biweekly Report","text":"<p>Subject: Beginning of the Project</p> <p>Project Title: BotWheels</p> <p>Report Number: 1</p> <p>Date: October 12, 2024</p> <p>Week Range: Weeks 1-2</p>"},{"location":"biweekly-reports/weekly-report-1/#1-progress-overview","title":"1. Progress Overview","text":"<p>During this period, tasks were completed to lay solid foundations for the entire project. The focus was on distributing tasks related to research and structuring work with the repository. Linters for Python (Ruff) and a markdown linter were introduced, unifying the workflow of the project.</p> <p>Setting up the local development environment for all team members was a main task, ensuring everyone could work on the project locally. Some team members committed to their tasks effectively, resulting in progress ahead of schedule in certain areas. However, delays were experienced in others.</p> <p>The team successfully diagnosed and preliminarily implemented visualizations of the bot's environment states, exceeding weekly project assumptions. This is expected to positively impact the implementation of an effective bot and the development of the PPO and A2C/A3C algorithms. An increase in the number of researched topics can be observed in the docs repository, which are being integrated into the core repository on separate branches. Weekly status meetings are conducted every Wednesday at 20:00.</p>"},{"location":"biweekly-reports/weekly-report-1/#2-team-member-contributions-marks-and-linked-issues","title":"2. Team Member Contributions, Marks, and Linked Issues","text":"Team Member Mark Summary of Contributions Linked Issues Jakub Ciso\u0144 4.5 Led the team and conducted research on the bot's gameplay in an online game, focusing on anti-cheat detection. Task distribution errors occurred, leading to some members not receiving assignments. The absence of a management system was identified and corrected, improving future task planning. Efforts will be made to better match task sizes for equitable workloads. - Research security systems of Rocket League and Anti-Cheat statements Kacper Drozdowski 3.5 Focused on understanding basic agent movement and producing a supporting document. A document about the A2C algorithm was expected for conversion to A3C, due October 14, 2024, but no committed work has been made as of October 13, 2024. - Implement Movement Testing Script for Bot Actions  - Research A2C to A3C transformation Mateusz Go\u015bciniecki 4 Implemented basic bot behaviors and delivered documentation as a foundation for advanced implementations. Assigned a task to implement the basic PPO algorithm but reported illness may delay this delivery. - Implement a Basic Bot That Drives Straight  - Set Up Basic PPO Training Anna Ivanytska 2 Tasked with implementing the basic DQN algorithm, but this was not delivered due to late communication of implementation issues. Prepared a detailed research document as groundwork for future implementation, but very simple. - Initial Implementation of the DQN Algorithm  - Documentation for the DQN Algorithm Igor Malkovskiy 4.5 Implemented basic bot behaviors and delivered documentation for future use. Assigned a basic PPO algorithm task, but illness may delay delivery. - Implement a Basic Bot That Drives Straight  - Set Up Basic PPO Training Dawid Mielewczyk 5 Demonstrated excellent preparation for project work, establishing a work culture, and setting up Discord bots for merge request notifications. Configured linters and established pipelines. - Verify and Optimize Project Configuration for Best Practices  - Familiarize with the PPO Algorithm  - Issue #35 Camille Nadir 3 Summarized knowledge about the PPO algorithm, it was delivered, but very late, so there was no time, to merge it till 14.10. Tasked with providing a detailed list of game environment states, which was also not completed. - Familiarize with the PPO Algorithm  - Create Documentation Entry Summarizing the PPO Algorithm Micha\u0142 Pryba 5 Showed strong commitment in researching the A3C algorithm and addressing implementation issues. Responsible for A3C algorithm implementation in the project. - Initial Implementation of the A3C Algorithm  - Investigate A3C Algorithm - only documentation for now Wojciech Szamocki 5 Demonstrated reliability by diagnosing possibilities for visualizing agent states and the environment. Initiated work on the core repository based on personal initiative. - Create visual mockup  - Develop (Transparent if possible) Overlay/Window Konrad Siemi\u0105tkowski 2 No task was assigned in the first week, which was the leader's responsibility. In the second week, a small task was assigned to improve the welcome page on GitHub docs, but this was not completed. - Write a Welcome Page for Documentation Micha\u0142 Zarzycki 3.5 Tasked with delivering and examining the implementation of the PPO algorithm, but this was not delivered. Redid the previously assigned task of creating a test environment. - Familiarize with the PPO Algorithm  - Test PPO Integration with the Game"},{"location":"biweekly-reports/weekly-report-1/#3-challenges-and-problems-encountered","title":"3. Challenges and Problems Encountered","text":"<p>The person responsible for the initial implementation of the DQN algorithm did not fulfill their task, leading to the decision to abandon this approach for now. As an equivalent, documentation was prepared on this approach, which will serve as a foundation for future implementation if sufficient time and resources become available.</p> <p>Some tasks are blocking the progress of other tasks, slowing down overall workflow and distribution. The engagement of group members is very varied, leading to communication problems and, as mentioned earlier, mutual blocking of tasks.</p>"},{"location":"biweekly-reports/weekly-report-1/#4-plans-for-the-next-period-weeks-3-4","title":"4. Plans for the Next Period (Weeks 3-4)","text":"<p>A better implementation strategy will be developed, and the distribution of human resources will be changed. Direct addressing of issues will be implemented to avoid blocking. A new task management system will be created based on the experience from the previous two weeks. A task planning system for the next few weeks will also be established. The A2C and PPO algorithms will be implemented, focusing on improving core functionalities.</p>"},{"location":"biweekly-reports/weekly-report-1/#5-summary","title":"5. Summary","text":"<p>In summary, the project started well but faced significant challenges during the first two weeks due to miscommunication regarding task assignments and varied engagement from team members. Plans for the next period aim to establish clearer roles and responsibilities to ensure smoother progress.</p>"},{"location":"biweekly-reports/weekly-report-2/","title":"Biweekly Report for the Bot Project","text":"<p>Subject: Initial Implementation of RL Algorithms and Addition of Fundamental Visualizations</p> <p>Project Title: BotWheels</p> <p>Report Number: 2</p> <p>Date: October 27, 2024</p> <p>Week Range: Weeks 3-4</p>"},{"location":"biweekly-reports/weekly-report-2/#1-progress-overview","title":"1. Progress Overview","text":"<p>Significant progress has been made with the development of base PPO and A2C/A3C algorithms, which are already yielding training results. Extensive state visualizations have been implemented with further expansions possible. Weekly goals are largely being met, with some even ahead of schedule, supporting a favorable outlook for project completion. A team member has proposed Rocket Learn, a tool from the RLGym community that facilitates advanced bot training. This tool has been assigned for parallel development as a potential approach.</p>"},{"location":"biweekly-reports/weekly-report-2/#2-team-member-contributions-marks-and-linked-issues","title":"2. Team Member Contributions, Marks, and Linked Issues","text":"Team Member Mark Summary of Contributions Linked Issues Jakub Ciso\u0144 4.5 Organized team work, adapted tasks based on feedback from project members, conducted code reviews. Occasionally experienced minor delays in delivering new tasks. Maintained communication with the Rocket League bot community to improve project work. Kacper Drozdowski 3.5 Conducted research on the A2C algorithm and implemented it for testing against a local player (delayed, scheduled for completion by 28.10.2024 with validation in the next report). - Research about A2C algorithm and how we can tranform that info A3C approche - Implement A2C/A3C Training Against Human Players Mateusz Go\u015bciniecki 5 Developed the main training loop for the PPO algorithm and assessed the feasibility of using Rocket Learn to achieve better results for bots. Ultimately responsible for the parallel implementation of the algorithm using Rocket Learn. - Rocket Learn: Creating a Basic Training Loop vs Bot Anna Ivanytska 3.5 Delivered delayed research on the DQN algorithm and began implementing an event logger visualization. - Create Event Log Window - Documentation for the DQN Algorithm Igor Malkovskiy 4 Delivered the PPO algorithm against another agent and conducted relevant research. Some delays encountered in meeting original deadlines. - Implement Training Against Any Other Agent Dawid Mielewczyk 5 Provided technical support within the project, particularly with PPO implementations. Regular code reviews and configured Redis for one of the RL algorithm implementations. - Support for Rocket-Learn (Bots) and Redis Setup Camille Nadir 3 Delivered a basic list of key Rocket League game states. An additional assigned task involved providing a training metric measurement tool, but no activity was registered for this task as of 27.10.2024. - Prepare Bot Performance Metric Tool - Prepare List of States and Priorities for Training Micha\u0142 Pryba 5 Implemented the A2C algorithm, allowing for adaptation to the final A3C version, and launched initial bot training sessions. - Support for A2C/A3C Implementation - Inital Implementation of the A3C Algorithm Wojciech Szamocki 5 Developed extensive in-game visualizations, including agent states, ball tracking, and additional views. - Create a Visualisation Konrad Siemi\u0105tkowski 4 Unified GitHub Pages with current research documents and is currently working on implementing the A2C/A3C algorithm for training against another bot. Some delays were encountered in meeting initial objectives. - Implement A2C/A3C Training Against Another Bot - Provide better file naming and add all unlinked files to mkdocs Micha\u0142 Zarzycki 5 Established a basic bot training setup and an initial reward system using the PPO algorithm. Exploring the possibility of training against a local player. - Implement PPO Training Against Human Players - Test PPO Integration with the Game"},{"location":"biweekly-reports/weekly-report-2/#3-challenges-and-problems-encountered","title":"3. Challenges and Problems Encountered","text":"<p>One primary challenge was task ambiguity, which will be addressed through clearer task descriptions. Task delivery delays from certain team members have occasionally slowed progress. Additionally, a need for training equipment, such as virtual machines and access to university resources, was identified.</p>"},{"location":"biweekly-reports/weekly-report-2/#4-plans-for-the-next-period-weeks-5-6","title":"4. Plans for the Next Period (Weeks 5-6)","text":"<p>Deliver refined RL algorithms that incorporate Rocket League-specific rewards. Visualization work will continue, with graphic adjustments and additional features as needed. A performance metric tool will be developed to meet project requirements and evaluate trained bots\u2019 effectiveness.</p>"},{"location":"biweekly-reports/weekly-report-2/#5-summary","title":"5. Summary","text":"<p>Project progress is on track, with issues being addressed as they arise.</p>"},{"location":"biweekly-reports/weekly-report-3/","title":"Biweekly Report for the Bot Project","text":"<p>Subject: Milestone I - Summary of Current Work, Further Implementation of Algorithms and Visualizations, Project Unification</p> <p>Project Title: BotWheels</p> <p>Report Number: 3</p> <p>Date: November 11, 2024</p> <p>Week Range: Weeks 5-6</p>"},{"location":"biweekly-reports/weekly-report-3/#1-progress-overview","title":"1. Progress Overview","text":"<p>The overall engagement from the team has decreased in recent weeks. Currently, we are focusing on unifying the project approach and developing a simple CLI to improve usability. At the same time, we are continuing to refine the PPO and A2C/A3C algorithms, with particular attention to improving the reward module for better performance.</p>"},{"location":"biweekly-reports/weekly-report-3/#2-team-member-contributions-marks-and-linked-issues","title":"2. Team Member Contributions, Marks, and Linked Issues","text":"Team Member Mark Summary of Contributions Linked Issues Jakub Ciso\u0144 4 Team management, leading conceptual work, preparing presentation for milestone 1. Limited availability led to slight delays in task delivery to other team members. Kacper Drozdowski 3 Delivered research on reward functions, though with significant delay from the declared date in report 2. Currently assigned to implement reward functions in the code. - Research about different reward functions - Implement Additional Reward Functions Mateusz Go\u015bciniecki 4 Continued work on the rocket-learn approach \u2013 backup approach. Moderate involvement with little produced code. If the approach does not meet expectations, it will be reassigned next week. - Rocket Learn: Creating a Basic Training Loop vs Bot Anna Ivanytska 3.5 Delivered event logger for visualization with delay. Currently working on more advanced approaches. Poor communication. - Expansion of Event Logger - https://github.com/bot-wheels/bot-wheels-core/issues/50 Igor Malkovskiy 2 Assigned task for training parameterization \u2013 no feedback over the last 2 weeks. - Transition Training Parameterization from In-Code to Config File Dawid Mielewczyk 4.5 Standardization of project work \u2013 code. Currently working on integrating CLI into the project. Technical support \u2013 review requests. - Develop CLI for Training and Gameplay Modes -Improving Work Structuring - Concept for Standardizing Variable Naming, Code Documentation, ETC Camille Nadir 2 Very poor communication over the last weeks. No solutions delivered. - Diagnostic Matches for Bot Performance Evaluation - Prepare Bot Performance Metric Tool Micha\u0142 Pryba 5 Very high engagement in the project. Offering own ideas. Continued work on the A2C algorithm. Implemented reward module into RL algorithms. - Implement A2C Algorithm with RLBot -Support for A2C/A3C Implementation Wojciech Szamocki 5 Very high engagement in the project. Offering own ideas. Delivered data logging for metrics to files and identified the visualization topic in bot training. Currently working on heatmap for bot activity on the field. - Heatmap of Bot and Opponent Activity -Save Visualization Data as CSV After Match Completion Konrad Siemi\u0105tkowski 4 Delivered research on optimization of learning parameters for A2C. Currently working on improving data collection for A2C and integrating game events. - Refine A2C Data Collection &amp; Integrate Game Event Metrics - #41 research and optimization of learning parameters of a2c Micha\u0142 Zarzycki 4.5 Work on PPO. Delivered logs to Google Drive. Delivered sample bot game result - recording. - Task: Fix Issues in CustomObsBuilder Class -Implement PPO Training Against Human Players"},{"location":"biweekly-reports/weekly-report-3/#3-challenges-and-problems-encountered","title":"3. Challenges and Problems Encountered","text":"<p>There have been communication issues with some team members, along with reduced overall engagement compared to the project's start. This has led to delays in delivering solutions. Additionally, some individuals have taken on significant responsibility for the team, resulting in a relatively large workload for them, but this has helped move the project forward.</p>"},{"location":"biweekly-reports/weekly-report-3/#4-plans-for-the-next-period-weeks-7-8","title":"4. Plans for the Next Period (Weeks 7-8)","text":"<p>In the next period, the goal is to deliver the CLI and a more effectively trained bot capable of consistently performing actions, such as scoring goals. The bot's performance will be evaluated based on predefined criteria leading to ensure it meets the expected outcomes. Initial measurements will be taken to compare the bot\u2019s behavior against the assumptions made during the planning stages, providing a foundation for further refinement and adjustments.</p>"},{"location":"biweekly-reports/weekly-report-3/#5-summary","title":"5. Summary","text":"<p>Improved self-motivation within the team is expected to lead to better coordination and more consistent progress. With increased individual drive, the team members will be more proactive in completing tasks and communicating effectively, which should help streamline the development process. This motivation is crucial for ensuring that everyone stays focused on their responsibilities and contributes to the overall success of the project.</p>"},{"location":"biweekly-reports/weekly-report-4/","title":"Biweekly Report for the Bot Project","text":"<p>Subject: Milestone I - Summary of Current Work, Further Implementation of Algorithms and Visualizations, Project Unification</p> <p>Project Title: BotWheels</p> <p>Report Number: 4</p> <p>Date: November 24, 2024</p> <p>Week Range: Weeks 7-8</p>"},{"location":"biweekly-reports/weekly-report-4/#1-progress-overview","title":"1. Progress Overview","text":"<p>The team is now almost entirely focused on training the models to achieve the project's defined goals. Available hardware resources were shared on the forum, and based on this information, specific team members were assigned to dedicate 100% of their efforts to training both models. Out of the initially planned algorithms (PPO, DQN, and A2C/A3C), only PPO and A2C remain, as implementation challenges led to the exclusion of DQN.</p> <p>Low engagement from most team members persists, likely due to overlapping deadlines for engineering theses and other projects. If possible, an extension of the project deadline would be strongly preferred. Although it is likely that the project will be delivered as a whole, achieving the primary goal\u2014a win rate of over 50% in 20 matches against Psyonix Rookie bots\u2014might be challenging. An additional few weeks would significantly increase the likelihood of reaching this objective. Nonetheless, the team will strive to meet the target within the current timeline, but this situation is being communicated at this stage.</p>"},{"location":"biweekly-reports/weekly-report-4/#2-team-member-contributions-marks-and-linked-issues","title":"2. Team Member Contributions, Marks, and Linked Issues","text":"Team Member Mark Summary of Contributions Linked Issues Jakub Ciso\u0144 4.5 Project leadership, meetings, management, and code reviews. Improved task allocation compared to the previous phase. Kacper Drozdowski 4 Implementation of the reward function module. Currently training the A2C model. - Implement Additional Reward Functions - A2C Initial Training &amp; Testing via Local Mateusz Go\u015bciniecki 4 Completed the implementation of the algorithm using Rocket Learn. Conducted research and is now working on establishing communication between shared hardware resources. - Remote Training Setup for Rocket League Bot Anna Ivanytska 3 Very poor communication. Over the past two weeks, it has been almost impossible to contact her. Currently working on improving the event logger. - Expansion of Event Logger Igor Malkovskiy 4 Introduced a configurable training parameter file instead of hardcoding values. Currently integrating the reward module with the PPO algorithm and training the model. - Transition Training Parameterization from In-Code to Config File - PPO Model Training and Testing Dawid Mielewczyk 4.5 Code reviews, technical support, and management assistance. Implemented the CLI and improved project structure. Had slightly limited availability due to a delegation, which was communicated in advance. - Develop CLI for Training and Gameplay Modes Camille Nadir 3.5 Relatively poor communication. Worked on implementing diagnostic metrics after gameplay. Currently training the PPO model. - PPO Model Training and Testing - Diagnostic Matches for Bot Performance Evaluation Micha\u0142 Pryba 5 Provided significant technical support, maintained high communication standards, and proactively identified project needs. Currently without a specific project task due to heavy involvement in earlier phases. - Technical Project Support Wojciech Szamocki 4.5 Added a heatmap to visualization. Currently working on enhancing model management and ensuring training continuity. - Enhance Model Management and Training Continuity - Heatmap of Bot and Opponent Activity Konrad Siemi\u0105tkowski 4 Refined data collection for the A2C algorithm and integrated game event metrics. Currently training the A2C model. - Initial A2C Model Training and Testing (via Remote Connection) Micha\u0142 Zarzycki 4.5 Integrated PPO into the project, introduced necessary changes and fixes to his code, and provided training resources. - Task: Fix Issues in CustomObsBuilder Class - Implement PPO Training Against Human Players"},{"location":"biweekly-reports/weekly-report-4/#3-challenges-and-problems-encountered","title":"3. Challenges and Problems Encountered","text":"<p>Communication and motivational issues persist. A lack of information flow often causes bottlenecks and delays in other tasks, though not universally, as this issue applies to specific cases. These challenges were already highlighted in the previous report.</p> <p>Training remains resource-intensive, and at times, the team struggles to maintain the continuity of training loops due to limited resources. To address this, a solution for saving checkpoints is currently being developed, which will allow training to resume in smaller segments. Additionally, it may be necessary to improve and expand reward functions to enhance model performance</p>"},{"location":"biweekly-reports/weekly-report-4/#4-plans-for-the-next-period-weeks-9-10","title":"4. Plans for the Next Period (Weeks 9-10)","text":"<p>In the upcoming weeks, the team will focus on delivering the project as a complete and functional solution. Efforts will be directed towards achieving the primary goal of a win rate of over 50% in 20 matches against Psyonix Rookie bots, ensuring the models are trained and tested to meet this target.</p> <p>Additionally, the project will be summarized, highlighting the approaches used, the results achieved, and the challenges encountered. The entire codebase will be documented in detail, including explanations of the structure, key functionalities, and usage instructions, to ensure it is clear, maintainable, and ready for presentation.</p> <p>If time allows, further refinements to reward functions and additional training adjustments will be considered to improve model performance and better align outcomes with project goals. The team will aim to balance these priorities to deliver a polished and well-prepared final product.</p>"},{"location":"biweekly-reports/weekly-report-4/#5-summary","title":"5. Summary","text":"<p>The team is making steady progress towards completing the project, with the main focus now on model training to meet the defined goals, particularly achieving a win rate of over 50% in matches against Psyonix Rookie bots. While challenges with communication and resource constraints have slowed progress, solutions are being implemented, including the development of checkpoint saving to manage training more efficiently. Despite these difficulties, the team remains committed to delivering the project in its complete form, with a well-documented codebase and a comprehensive project summary. The next period will focus on finalizing the training, improving model performance, and ensuring the project is ready for delivery.</p>"},{"location":"biweekly-reports/weekly-report-5/","title":"Biweekly Report for the Bot Project","text":"<p>Subject: Milestone II - Model Training, Pipeline Refinement, and Final Project Adjustments</p> <p>Project Title: BotWheels</p> <p>Report Number: 5</p> <p>Date: December 9, 2024</p> <p>Week Range: Weeks 9-11</p>"},{"location":"biweekly-reports/weekly-report-5/#1-progress-overview","title":"1. Progress Overview","text":"<p>Due to the approaching deadline for thesis submissions, overall team engagement in the project has decreased, which is understandable. Currently, the focus remains on training the models and refining the main project pipeline. The expected deliverables include fully trained models, detailed documentation for the training process, and well-commented code to ensure clarity and maintainability.</p> <p>The training of both models is progressing, but delays are anticipated due to the reduced availability of some team members. Despite these challenges, efforts are ongoing to meet the key milestones, and adjustments are being made to optimize the model's performance.</p>"},{"location":"biweekly-reports/weekly-report-5/#2-team-member-contributions-marks-and-linked-issues","title":"2. Team Member Contributions, Marks, and Linked Issues","text":"Team Member Mark Summary of Contributions Linked Issues Jakub Ciso\u0144 4 Continued managing communication within the team, although contributions have been lower over the past three weeks due to other commitments. Kacper Drozdowski 4 More active compared to previous weeks, currently training the bot using the A2C algorithm. - A2C Initial Training &amp; Testing via Local Mateusz Go\u015bciniecki 3 Reduced communication in the past few weeks. Currently training the PPO model. - Remote Training Setup for Rocket League Bot Anna Ivanytska 3 Very poor communication, significant absence from meetings. - Expansion of Event Logger - Add/Update Documentation According to Standardization Guidelines Igor Malkovskiy 3 Reduced communication in the past few weeks. - PPO Model Training and Testing Dawid Mielewczyk 4 Waiting for descriptions from other team members to complete the CLI. Available for code reviews and other support tasks. - Develop CLI for Training and Gameplay Modes Camille Nadir 3 Very poor communication, significant absence from meetings, currently working on training the bot using PPO. - PPO Model Training and Testing Micha\u0142 Pryba 5 Significant contribution to the project, currently providing support. - Technical project support Wojciech Szamocki 4.5 Reduced activity relative to previous weeks, but communicated about travel. Continues to deliver quality code, currently working on model management. - Enhance Model Management and Training Continuity Konrad Siemi\u0105tkowski 3.5 Training the A2C model, limited communication. - Initial A2C Model Training and Testing (via Remote Connection) Micha\u0142 Zarzycki 3.5 Reduced communication. Currently expected to add a \"how-to\" guide for his code. - Add (How-to) .md for Running Bot Training with PPO"},{"location":"biweekly-reports/weekly-report-5/#3-challenges-and-problems-encountered","title":"3. Challenges and Problems Encountered","text":"<p>The improper distribution of tasks during the initial weeks has impacted the latter stages, which coincide with thesis submission deadlines. As a result, there has been less time dedicated to the project, with fewer people actively engaged. Communication issues remain the primary challenge, which has led to delays and inefficiencies in task completion.</p>"},{"location":"biweekly-reports/weekly-report-5/#4-plans-for-the-next-period","title":"4. Plans for the Next Period","text":"<p>As established, the final submission of the project will occur during the wrap-up session. By that time, fully trained models are expected to be delivered, along with completed tests and any necessary adjustments. Additionally, the project documentation will be finalized to provide a clear overview of the model training process and final results.</p>"},{"location":"biweekly-reports/weekly-report-5/#5-summary","title":"5. Summary","text":"<p>Despite challenges related to reduced availability and communication issues, progress is being made toward the completion of the project. The team is focused on finalizing the model training and refining the project pipeline to meet the established goals. The upcoming period will be critical for delivering the final results, ensuring the models are trained as expected, and preparing the project for final presentation.</p>"},{"location":"project-guide/Properly-BOT-training/","title":"Guide how to train Rocket League Bot","text":"<p>To start training the bot you have to run A2C.py file. Every reward is defined in rewards.py file. The weights and parameters of every single reward is defined in reward-config.json file. To run the visual simulation you have to enter RocketSimVis folder in your project and open RUN file.</p>"},{"location":"project-guide/Properly-BOT-training/#early-stages-of-training-bronze-silver","title":"Early Stages of Training (Bronze - Silver)","text":""},{"location":"project-guide/Properly-BOT-training/#defining-early-stages","title":"Defining Early Stages","text":"<p>The early stages of training refer to the period when your bot is not yet trying to score goals. At this stage, bots typically cannot push or shoot the ball into the goal. The primary focus should be on:</p> <ol> <li>Learning to touch the ball (TouchBallReward)</li> <li>Ensuring the bot doesn't forget how to jump (FlipReward)</li> </ol>"},{"location":"project-guide/Properly-BOT-training/#why-do-bots-forget-how-to-jump","title":"Why Do Bots Forget How to Jump","text":"<p>Controlling the car in the air is challenging and less forgiving compared to ground driving. A fresh bot tasked with reaching the ball often learns quickly to avoid jumping altogether. Unfortunately, this makes it harder for the bot to rediscover jumping later.</p>"},{"location":"project-guide/Properly-BOT-training/#rewards-for-early-stages","title":"Rewards for Early Stages","text":"<p>Here are some effective rewards to help a fresh bot learn to hit the ball quickly:</p> <pre><code>REWARD_FUNCTIONS = {\n    \"TouchBallReward\": TouchBallReward, # Giant reward for actually hitting the ball\n    \"FlipReward\": FlipReward, # Jumping reward\n    \"BallPossessionReward\": BallPossessionReward, # Possesing the ball\n    \"DistanceToBallReward\": DistanceToBallReward, # Move towards the ball\n}\n</code></pre> <p>Note: Avoid rewards for scoring or moving the ball toward the goal at this stage. These add noise and slow down learning.</p>"},{"location":"project-guide/Properly-BOT-training/#weights","title":"Weights","text":"<p>Try to increase the weights of touching the Ball first! After a few dozen million steps, your bot should frequently hit the ball.</p>"},{"location":"project-guide/Properly-BOT-training/#learning-to-score","title":"Learning to Score","text":"<p>Once your bot reliably hits the ball, introduce rewards for moving the ball toward the goal and scoring. Decrease the TouchBallReward significantly so it is no longer the bot's primary objective.</p>"},{"location":"project-guide/Properly-BOT-training/#recommended-rewards","title":"Recommended Rewards","text":"<ul> <li>VelocityBallToGoalReward: Use this for continuous encouragement to move the ball toward the goal. </li> </ul>"},{"location":"project-guide/Properly-BOT-training/#warning-avoid-massive-goal-rewards","title":"Warning Avoid Massive Goal Rewards","text":"<p>Avoid assigning excessive weight to goal rewards, such as:</p> <pre><code> \"name\": \"GoalReward\",\n        \"params\": {\n            \"weight\": 10\n          },\n          \"weight\": 100.0 \n</code></pre>"},{"location":"project-guide/Properly-BOT-training/#why","title":"Why","text":"<p>Massive goal rewards drown out other rewards, reducing exploration and slowing learning. Instead, use a reasonable weight (e.g., 10 or 20) for goal rewards. Note that a bot can be trained to high levels without goal rewards at all.</p>"},{"location":"project-guide/Properly-BOT-training/#middle-stages-gold-plat","title":"Middle Stages (Gold - Plat)","text":""},{"location":"project-guide/Properly-BOT-training/#defining-middle-stages","title":"Defining Middle Stages","text":"<p>Once your bot can push the ball into the net, it enters the middle stages. This stage involves more complexity and refinement.</p>"},{"location":"project-guide/Properly-BOT-training/#objectives","title":"Objectives","text":"<ol> <li>Basic shots</li> <li>Basic saves</li> <li>Simple jump-touches and aerials</li> <li>Effective 50-50s</li> <li>Boost collection and management</li> <li>Giving space to teammates (if not 1v1)</li> </ol>"},{"location":"project-guide/Properly-BOT-training/#improved-ball-touch-reward","title":"Improved Ball-Touch Reward","text":"<p>The default touch reward from TouchBallReward becomes less effective as bots improve. To address this, scale the reward based on the strength of the touch. For example:</p> <ol> <li>Use <code>ball_touched</code> (a property of players) to detect ball interactions.</li> <li>Calculate the ball\u2019s velocity change between steps.</li> </ol>"},{"location":"project-guide/Properly-BOT-training/#encouraging-air-touches","title":"Encouraging Air Touches","text":"<p>Bots often fear the air, requiring encouragement to hit the ball high up.\u00a0 Combine time spent in the air with the ball\u2019s height:</p> <pre><code>MAX_TIME_IN_AIR = 1.75  # Maximum aerial time\nair_time_frac = min(player.air_time, MAX_TIME_IN_AIR) / MAX_TIME_IN_AIR\nheight_frac = ball.position[2] / CommonValues.CEILING_Z\nreward = min(air_time_frac, height_frac)\n</code></pre>"},{"location":"project-guide/Properly-BOT-training/#boost-management","title":"Boost Management","text":""},{"location":"project-guide/Properly-BOT-training/#encourage-boost-pickup","title":"Encourage Boost Pickup","text":"<p>Use EventReward\u2019s boost_pickup for rewarding boost collection. Increase the reward for small pads to ensure bots don\u2019t ignore them.</p>"},{"location":"project-guide/Properly-BOT-training/#encourage-boost-conservation","title":"Encourage Boost Conservation","text":"<p>Reward bots based on boost levels, emphasizing lower amounts:</p> <pre><code>reward = sqrt(player.boost_amount)\n</code></pre>"},{"location":"project-guide/Properly-BOT-training/#tips","title":"Tips","text":"<ul> <li>Increase the reward if the bot wastes boost.</li> <li>Decrease it if the bot hoards boost unnecessarily.</li> </ul>"},{"location":"project-guide/Properly-BOT-training/#developing-outplays","title":"Developing Outplays","text":"<p>Outplays are critical for advancing to later stages. This involves mechanics like:</p> <ul> <li>Dribbling and flicking (common among bots)</li> <li>Cuts and passes</li> <li>Air dribbles and flip resets</li> </ul>"},{"location":"project-guide/Properly-BOT-training/#custom-rewards","title":"Custom Rewards","text":"<p>Introduce specific rewards for mechanics you want your bot to learn. For example, if teaching flip resets, reward proper alignment, car positioning, and ball contact during a reset.</p>"},{"location":"project-guide/Properly-BOT-training/#conclusion","title":"Conclusion","text":"<p>By following these steps, you can guide your bot from basic ball touches to advanced mechanics and competitive gameplay. Each stage builds on the previous one, ensuring steady and meaningful progress. Use the provided rewards and techniques as a foundation, and feel free to experiment to achieve the behavior you desire.</p>"},{"location":"project-guide/Running-Bot-Training-with-PPO/","title":"Documentation for Running Bot Training with PPO","text":""},{"location":"project-guide/Running-Bot-Training-with-PPO/#introduction","title":"Introduction","text":"<p>This documentation provides clear instructions on how to run bot training using the PPO algorithm. It outlines all necessary steps, configurations, and dependencies required for successful execution. Below, you will find prerequisites to set up the environment, detailed configuration steps, and guidance on initiating the training process.</p>"},{"location":"project-guide/Running-Bot-Training-with-PPO/#prerequisites","title":"Prerequisites","text":"<ul> <li>You need the following libraries to run the bot:</li> <li>configparser</li> <li>os</li> <li>rlgym</li> <li>stable_baselines3 (PPO)</li> <li>pynput (keyboard)</li> <li>gym.spaces</li> <li>numpy</li> <li>pathlib</li> <li>The bot relies on a custom <code>GoogleDriveManager</code> (from <code>utils.drive_manager</code> import <code>GoogleDriveManager</code>).  </li> <li>A credentials.json file is required, containing secrets to connect to Google Drive.  </li> <li>Users who want to run the script must share their email to gain access.  </li> <li>For inquiries related to Google Drive, contact @Assassin-PL.</li> </ul>"},{"location":"project-guide/Running-Bot-Training-with-PPO/#configuration-steps","title":"Configuration Steps","text":"<ol> <li> <p>Install Dependencies    Make sure you have all required libraries installed. A virtual environment (e.g., venv or conda) is highly recommended to keep dependencies organized.</p> </li> <li> <p>Create the Agent Instance    Instantiate the <code>SimpleAgent</code> class. This agent automatically loads any previously trained model if one is found:</p> </li> </ol> <p>```python    from agent import SimpleAgent</p> <p>agent = SimpleAgent()</p> <p>```</p> <ol> <li> <p>Configure Bot Settings    In the <code>settings.cfg</code> file, adjust the bot's parameters to define how the training loop behaves. For example, you can modify episode lengths, reward function details, or specific environment constants. Any change here affects how the bot explores and learns within the PPO framework.</p> </li> <li> <p>Initiate Training    After configuring your environment and creating the <code>SimpleAgent</code> instance, start the training loop by calling:</p> </li> </ol> <p><code>python    agent.train()</code></p> <p>This process leverages any existing model, retrains it with the updated settings, and logs progress details.</p>"},{"location":"project-guide/Running-Bot-Training-with-PPO/#training-execution","title":"Training Execution","text":"<p>When you call <code>agent.train()</code>, the following actions occur:</p> <ul> <li>The bot environment (defined via <code>rlgym</code> and your configuration) initializes using the <code>make_env()</code> method. It's important to note that PPO requires a specific environment setup; the <code>make_env()</code> method cannot create a training environment if, for example, the bot was previously trained to play against another bot or in a training loop reacting to human input.</li> <li>The previously trained model (if available) is loaded to continue its training.</li> <li>Data (observations, rewards, etc.) is continuously collected, and the agent updates its policy using PPO.</li> <li>Periodically or upon completion, the agent saves checkpoints of the model.</li> </ul> <p>Configuration Note:</p> <p>Ensure that the <code>make_env()</code> function correctly returns the environment (<code>env</code>). All accessible variables and parameters for configuring the training loop are located in the <code>settings.cfg</code> file. Only make changes within this file to alter training configurations, as everything else has been preconfigured.</p>"},{"location":"project-guide/Running-Bot-Training-with-PPO/#parameters-and-their-effects","title":"Parameters and Their Effects","text":"<p>In the <code>settings.cfg</code> file and within your code, you can adjust various parameters. Key parameters include:</p> <ul> <li> <p>realtime: Boolean (<code>true</code>/<code>false</code>) that determines whether to run training in accelerated mode. Setting to <code>true</code> causes game time to be sped up (e.g., 1 second of real time corresponds to 1000 seconds in the game).</p> </li> <li> <p>modelName: String that specifies the name of the model being used or saved. This allows easy management of different training models.</p> </li> <li> <p>timesteps: Number of steps that defines how many environment steps occur before a model update. Adjusting this can affect the frequency and efficiency of policy updates.</p> </li> <li> <p>human: Boolean that specifies whether the bot is being trained to play against a human player. Setting this to <code>true</code> enables training scenarios where the bot competes against human input, enhancing its adaptability.</p> </li> </ul> <p>Tweaking these parameters can lead to different training outcomes in terms of speed, stability, and bot performance.</p>"},{"location":"project-guide/Running-Bot-Training-with-PPO/#match-configuration","title":"Match Configuration","text":"<p>Based on project assumptions, only the possibility to play in 1 vs 1 matches is enabled by default. This is configured by setting the match settings in the environment file to 1 vs 1.</p> <p>However, if you wish to enable games with more than 1 vs 1, you can adjust the match settings accordingly. This allows for more complex training scenarios involving multiple agents. For detailed instructions on configuring multiple agents, refer to the Multiple Agents section in the rlgym documentation.</p>"},{"location":"project-guide/Running-Bot-Training-with-PPO/#additional-information","title":"Additional Information","text":"<ul> <li>Automatic Model Upload: When training finishes (or at certain checkpoints), the final model can be automatically pushed to Google Drive. Ensure you have set up <code>credentials.json</code> and provided the necessary access for your email.</li> <li>Monitoring: Check logs and console output for training progress. This can help identify if the agent is converging or if adjustments to parameters are needed.</li> <li>Further Reading: Refer to <code>stable_baselines3</code> documentation for advanced PPO configurations, or consult <code>rlgym</code> documentation to customize the training environment further.</li> <li>Development Origin: The bot was developed following the instructions from the creators of <code>rlgym</code> (https://rlgym.org/docs-page.html#tutorials) and was modeled after the sample bot from the repository: https://github.com/RLBot/RLBotPythonExample.</li> </ul>"},{"location":"project-guide/codebase-standardization-guidelines/","title":"Codebase Standardization Guidelines","text":""},{"location":"project-guide/codebase-standardization-guidelines/#introduction","title":"Introduction","text":"<p>To improve code consistency, readability, and facilitate future development, we are introducing a unified concept for standardizing variable naming conventions, code documentation, and file organization across the project.</p>"},{"location":"project-guide/codebase-standardization-guidelines/#goals","title":"Goals","text":"<ul> <li>Establish a unified variable naming convention.</li> <li>Define a standard approach to in-code documentation.</li> <li>Ensure consistency in file structure and function naming across the repository.</li> </ul>"},{"location":"project-guide/codebase-standardization-guidelines/#naming-conventions","title":"Naming Conventions","text":""},{"location":"project-guide/codebase-standardization-guidelines/#variables-and-functions","title":"Variables and Functions","text":"<ul> <li>Use snake_case for variable and function names.</li> <li>Example: <code>user_name</code>, <code>calculate_total()</code></li> </ul>"},{"location":"project-guide/codebase-standardization-guidelines/#constants","title":"Constants","text":"<ul> <li>Use UPPER_SNAKE_CASE for constants.</li> <li>Example: <code>MAX_RETRIES</code>, <code>API_ENDPOINT</code></li> </ul>"},{"location":"project-guide/codebase-standardization-guidelines/#classes","title":"Classes","text":"<ul> <li>Use PascalCase (also known as UpperCamelCase) for class names.</li> <li>Example: <code>UserProfile</code>, <code>DataProcessor</code></li> </ul>"},{"location":"project-guide/codebase-standardization-guidelines/#file-naming","title":"File Naming","text":"<ul> <li>All file names in the should follow the <code>bot-wheels-core</code> repository snake_case convention.</li> <li>Example: <code>user_profile.py</code>, <code>data_processor.py</code></li> <li>For docs we are sticking with kebab-case.</li> <li>Example: <code>codebase-standardization-guidelines.md</code>, <code>match-against-ai-bots.md</code></li> </ul>"},{"location":"project-guide/codebase-standardization-guidelines/#in-code-documentation","title":"In-Code Documentation","text":"<ul> <li>Docstrings:</li> <li>Use triple double-quotes <code>\"\"\"</code> for module, class, and function/method docstrings.</li> <li>Follow the PEP 257 conventions.</li> <li> <p>Include descriptions of the function's purpose, arguments, return values, and exceptions.</p> </li> <li> <p>Inline Comments:</p> </li> <li>Use sparingly to explain non-obvious code.</li> <li>Begin with <code>#</code> followed by a space.</li> <li>Place above the code line or at the end if the comment is brief.</li> </ul>"},{"location":"project-guide/codebase-standardization-guidelines/#example-of-a-function-with-docstring","title":"Example of a Function with Docstring","text":"<pre><code>def calculate_total(price, quantity):\n    \"\"\"\n    Calculate the total price for a number of items.\n\n    Args:\n        price (float): The price of a single item.\n        quantity (int): The number of items purchased.\n\n    Returns:\n        float: The total price calculated as price multiplied by quantity.\n    \"\"\"\n    return price * quantity\n</code></pre>"},{"location":"project-guide/codebase-standardization-guidelines/#file-organization","title":"File Organization","text":"<ul> <li>Directory Structure:</li> <li>Organize files into directories based on features or functionality.</li> <li> <p>Use <code>__init__.py</code> files to define Python packages.</p> </li> <li> <p>Module Structure:</p> </li> <li>Each module should have a clear purpose and contain related classes and functions.</li> <li>Avoid circular dependencies by carefully structuring imports.</li> </ul>"},{"location":"project-guide/codebase-standardization-guidelines/#steps-for-implementation","title":"Steps for Implementation","text":""},{"location":"project-guide/codebase-standardization-guidelines/#1-review-current-codebase","title":"1. Review Current Codebase","text":"<ul> <li>Action: Conduct a comprehensive review of existing naming conventions, documentation styles, and file organization.</li> <li>Goal: Identify inconsistencies and areas needing refactoring.</li> </ul>"},{"location":"project-guide/codebase-standardization-guidelines/#2-concept-development","title":"2. Concept Development","text":"<ul> <li>Action: Create detailed guidelines (this document) for naming, documentation, and file structure.</li> <li>Goal: Establish clear standards for the team to follow.</li> </ul>"},{"location":"project-guide/codebase-standardization-guidelines/#3-team-alignment","title":"3. Team Alignment","text":"<ul> <li>Action: Present the guidelines to the team for feedback and discussion.</li> <li>Goal: Achieve consensus and ensure everyone understands the new standards.</li> </ul>"},{"location":"project-guide/codebase-standardization-guidelines/#4-implementation","title":"4. Implementation","text":"<ul> <li>Action: Plan and execute the refactoring of existing code according to the new standards.</li> <li>Goal: Update the codebase to be consistent and maintainable.</li> </ul>"},{"location":"project-guide/codebase-standardization-guidelines/#conclusion","title":"Conclusion","text":"<p>Adopting these standardized conventions will enhance code readability and maintainability, making it easier for all team members to collaborate effectively. Consistency in coding practices is essential for the long-term success of our project.</p>"},{"location":"project-guide/google-drive-manager/","title":"Usage Instructions for the <code>GoogleDriveManager</code> Class","text":"<p>The following instructions demonstrate how to use the <code>GoogleDriveManager</code> class to manage files on Google Drive. This class allows you to download, edit, and delete files in a selected Google Drive folder.</p>"},{"location":"project-guide/google-drive-manager/#1-initializing-the-google-drive-manager","title":"1. Initializing the Google Drive Manager","text":"<p>To use the class, you must first initialize it, which will authorize access to Google Drive:</p> <pre><code>from DriveManager import GoogleDriveManager\n\ndrive_manager = GoogleDriveManager()\n</code></pre> <p>Access to the files is granted by @Assassin-PL, who is the author of this code and the project administrator on Google Cloud. Users must be manually added as testers by @Assassin-PL and will receive the <code>credentials.json</code> file from them. The <code>token.pickle</code> file ensures that once the user has been added and the script has run successfully, there is no need to log in every time the script is used. After being added and running the script for the first time, users will be able to seamlessly download and interact with Google Drive within the entire project area by properly importing the <code>GoogleDriveManager</code> class.</p>"},{"location":"project-guide/google-drive-manager/#2-downloading-a-file-from-google-drive","title":"2. Downloading a File from Google Drive","text":"<p>To download a file from Google Drive, you can use the <code>open_file()</code> method, which returns the content of the file with the given name:</p> <pre><code>file_name = 'example.csv'\nfile_content = drive_manager.open_file(file_name)\nif file_content:\n    print(file_content.decode('utf-8'))\nelse:\n    print(f\"Plik '{file_name}' nie istnieje.\")\n</code></pre> <p>This method verifies whether a file with the specified name exists within the folder and then retrieves its contents.</p>"},{"location":"project-guide/google-drive-manager/#3-uploading-a-file-to-google-drive","title":"3. Uploading a File to Google Drive","text":"<p>To upload a file to Google Drive, you can use the <code>upload_file()</code> method. If a file with the same name already exists, it will be updated. This method handles the necessary authorization and ensures that the file is correctly placed within the designated Google Drive folder.</p> <pre><code>file_name = 'example.csv'\nfile_data = b\"id,first_name,last_name\\n1,John,Doe\"\nmime_type = 'text/csv'\n\ndrive_manager.upload_file(file_name, file_data, mime_type)\n</code></pre> <p>If the file already exists, its content will be updated; otherwise, a new file will be created.</p>"},{"location":"project-guide/google-drive-manager/#4-creating-a-new-file","title":"4. Creating a New File","text":"<p>To create a new file in Google Drive, you can use the <code>create_file()</code> method, which works similarly to <code>upload_file()</code>:</p> <pre><code>drive_manager.create_file(file_name, file_data, mime_type)\n</code></pre> <p>This method always creates a new file with the specified name. Ensure that the file name is unique within the target folder to avoid conflicts.</p>"},{"location":"project-guide/google-drive-manager/#5-listing-files","title":"5. Listing Files","text":"<p>To display a list of files in a Google Drive folder, you can use the <code>list_files()</code> method:</p> <pre><code>files = drive_manager.list_files()\nfor file in files:\n    print(f\"Nazwa: {file['name']}, ID: {file['id']}\")\n</code></pre> <p>This method retrieves a list of files along with their names and identifiers. It provides an organized overview of all files within the specified folder, enabling easy access and management of your Google Drive contents.</p>"},{"location":"project-guide/google-drive-manager/#6-deleting-a-file","title":"6. Deleting a File","text":"<p>To delete a file from Google Drive, you must first obtain its identifier using the <code>get_file_id()</code> method, and then utilize the <code>delete_file()</code> method:</p> <pre><code>file_id = drive_manager.get_file_id(file_name)\nif file_id:\n    drive_manager.delete_file(file_id)\n    print(f\"Plik '{file_name}' zosta\u0142 usuni\u0119ty.\")\nelse:\n    print(f\"Plik '{file_name}' nie zosta\u0142 znaleziony.\")\n</code></pre> <p>This process involves retrieving the unique identifier of the desired file, which ensures that the correct file is targeted for deletion. Once the file ID is obtained, the <code>delete_file()</code> method removes the file from the specified Google Drive folder. This functionality allows for precise management of your Google Drive contents, enabling you to maintain an organized and up-to-date file structure by removing unnecessary or outdated files.</p> <p>Note: Ensure that you have the necessary permissions to delete files and that you have correctly obtained the file ID to prevent accidental removal of important files.</p>"},{"location":"project-guide/google-drive-manager/#7-example-usage","title":"7. Example Usage","text":"<p>To demonstrate the capabilities of the <code>GoogleDriveManager</code> class, also you should download <code>data_generator</code> for this example only, the following example showcases how to check for the existence of a file, download it, edit its contents, and then upload it back to Google Drive:</p> <pre><code>from utils.drive_manager import GoogleDriveManager\nfrom data_generator import DataGen  # Assuming DataGen is the correct class for data generation\n\n# Initialize the Google Drive Manager\ndrive_manager = GoogleDriveManager()\nfile_name = 'dane.csv'\n\n# Check if the file exists in Google Drive\nfile_id = drive_manager.get_file_id(file_name)\n\nif file_id:\n    # File exists, download and edit its content\n    file_content = drive_manager.open_file(file_name)\n    csv_content = file_content.decode('utf-8')  # Decode the file content from bytes to string\n\n    # Initialize the data generator and load existing data from CSV\n    data = DataGen()\n    data.from_csv_string(csv_content)  # Load data from the CSV string\n    data.edit_data()  # Perform desired edits to the data\n    updated_csv_content = data.to_csv_string().encode('utf-8')  # Convert the updated data back to CSV string and encode to bytes\n\n    # Upload the updated file back to Google Drive, replacing the existing file\n    drive_manager.upload_file(file_name, updated_csv_content, 'text/csv')\nelse:\n    # File does not exist, create a new one\n    data = DataGen()\n    data.generate_data(num_records=20)  # Generate new data with 20 records\n    csv_content = data.to_csv_string().encode('utf-8')  # Convert the generated data to CSV string and encode to bytes\n\n    # Create a new file in Google Drive with the generated CSV content\n    drive_manager.create_file(file_name, csv_content, 'text/csv')\n</code></pre>"},{"location":"project-guide/poetry/","title":"Poetry Usage Tutorial","text":"<p>This tutorial will guide you through the essential steps to use a repository with Poetry. We'll cover installing dependencies, activating the shell, and adding/searching for packages.</p>"},{"location":"project-guide/poetry/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have Poetry installed. Use the appropriate command for your operating system:</p>"},{"location":"project-guide/poetry/#windows-powershell","title":"Windows (powershell)","text":"<pre><code>(Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -\n</code></pre>"},{"location":"project-guide/poetry/#macos-with-homebrew","title":"macOS (with Homebrew)","text":"<pre><code>brew install poetry\n</code></pre>"},{"location":"project-guide/poetry/#linux","title":"Linux","text":"<pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre>"},{"location":"project-guide/poetry/#step-1-installing-dependencies","title":"Step 1: Installing Dependencies","text":"<p>To install the dependencies for your project, navigate to the project directory and run:</p> <pre><code>poetry install\n</code></pre> <p>This command reads the <code>pyproject.toml</code> file in your project directory and installs the specified dependencies.</p>"},{"location":"project-guide/poetry/#step-2-activating-the-virtual-environment","title":"Step 2: Activating the Virtual Environment","text":"<p>Poetry creates a virtual environment for your project. To activate this environment, you can use the <code>poetry shell</code> command:</p> <pre><code>poetry shell\n</code></pre> <p>This command activates the virtual environment, allowing you to run commands in an isolated environment specific to your project.</p>"},{"location":"project-guide/poetry/#step-3-adding-packages","title":"Step 3: Adding Packages","text":"<p>To add a package to your project, use the <code>poetry add</code> command followed by the package name. For example, to add <code>requests</code> to your project:</p> <pre><code>poetry add requests\n</code></pre> <p>Poetry will automatically update your <code>pyproject.toml</code> file and install the package.</p>"},{"location":"project-guide/poetry/#step-4-searching-for-packages","title":"Step 4: Searching for Packages","text":"<p>To search for packages, you can use the <code>poetry search</code> command followed by the search query. For example, to search for packages related to \"flask\":</p> <pre><code>poetry search flask\n</code></pre> <p>This command will list all packages related to \"flask\" available on PyPI.</p>"},{"location":"project-guide/poetry/#summary","title":"Summary","text":"<ul> <li>Install dependencies: <code>poetry install</code></li> <li>Activate virtual environment: <code>poetry shell</code></li> <li>Add a package: <code>poetry add &lt;package-name&gt;</code></li> <li>Search for packages: <code>poetry search &lt;query&gt;</code></li> </ul> <p>By following these steps, you can efficiently manage your project's dependencies and environment using Poetry. For more detailed information, you can refer to the official Poetry documentation.</p>"},{"location":"project-guide/rlgym_ppo/","title":"Installation of RLGym-PPO and rlgym-sim","text":""},{"location":"project-guide/rlgym_ppo/#step-1-install-rlgym","title":"Step 1: Install RLGym","text":"<p>To install RLGym and ensure it works, follow the instructions in the installation guide.</p>"},{"location":"project-guide/rlgym_ppo/#step-2-install-rlgym-ppo-and-rlgym-sim","title":"Step 2: Install RLGym-PPO and rlgym-sim","text":"<p>Follow these steps to install all necessary components.</p> <ol> <li> <p>Install the RocketSim package using:</p> <p><code>sh pip install rocketsim</code></p> </li> <li> <p>Install the rlgym_sim package using:</p> <p><code>sh pip install git+https://github.com/AechPro/rocket-league-gym-sim@main</code></p> </li> <li> <p>Download the asset dumper from here and follow its instructions to create the <code>collision_meshes</code> folder (we will move it later).</p> </li> <li>If you have an NVIDIA graphics card, install CUDA from the CUDA Toolkit Archive.</li> <li>Install PyTorch from the official website. If you installed CUDA, choose the CUDA version; otherwise, choose the CPU version.</li> <li> <p>Install RLGym-PPO using:</p> <p><code>sh pip install git+https://github.com/AechPro/rlgym-ppo</code></p> </li> <li> <p>Move <code>collision_meshes</code> to your bot's folder.</p> </li> <li>Copy <code>example.py</code> from RLGym-PPO and verify it works. You can find the <code>example.py</code> file here.</li> </ol>"},{"location":"project-guide/rlgym_ppo/#step-3-visualize-using-rocketsimvis","title":"Step 3: Visualize Using RocketSimVis","text":"<p>To visualize the simulation, you can use the RocketSimVis tool. Follow the instructions in the RocketSimVis repository to set it up.</p>"},{"location":"project-guide/working-with-project/","title":"Working with the Repository","text":"<p>We use Git Flow to manage our development process, ensuring a structured and efficient workflow. For more details, refer to this Git Flow cheatsheet.</p>"},{"location":"project-guide/working-with-project/#branches","title":"Branches","text":"<ul> <li> <p>Main Branch: The main branch (<code>main</code>) holds the stable version of the project. Only key team members (me and Dawid) can merge into the <code>main</code> branch unless otherwise specified.</p> </li> <li> <p>Development Branch: The development branch (<code>develop</code>) is where active development happens. All feature branches are created from <code>develop</code> and merged back into it when a feature is completed. Only Dawid and I are authorized to merge into <code>develop</code> as well.</p> </li> </ul>"},{"location":"project-guide/working-with-project/#feature-branches","title":"Feature Branches","text":"<p>When starting a new feature, create a branch from <code>develop</code>:</p> <pre><code>git checkout develop\ngit pull origin develop\ngit checkout -b feature/my-new-feature\n</code></pre> <p>All feature branches should be named using the following pattern: <code>feature/&lt;description-of-feature&gt;</code>.</p>"},{"location":"project-guide/working-with-project/#merging","title":"Merging","text":"<p>Only authorized team members (Jakub C. and Dawid W.) can merge changes into <code>main</code> and <code>develop</code>. You can work on a feature branch, but a pull request must be reviewed before merging.</p>"},{"location":"project-guide/working-with-project/#commit-guidelines","title":"Commit Guidelines","text":"<p>To track progress, it is important to commit frequently, ideally every day. Use the Gitmoji convention to make commit messages more descriptive. For example:</p> <ul> <li>\ud83d\udea7 <code>:construction:</code> for work in progress.</li> <li>\u2728 <code>:sparkles:</code> for introducing a new feature.</li> <li>\ud83d\udc1b <code>:bug:</code> for fixing a bug.</li> </ul> <p>If a task is still in progress, use the \ud83d\udea7 emoji so that it\u2019s clear the feature is incomplete.</p> <p>Here\u2019s an example of a commit message:</p> <pre><code>git commit -m \":sparkles: Add new feature for calculating goals\"\n</code></pre>"},{"location":"project-guide/rocket-learn-setup/rocket-learn-setup/","title":"Rocket-learn setup","text":""},{"location":"project-guide/rocket-learn-setup/rocket-learn-setup/#installing-redis-on-windows","title":"Installing Redis on Windows","text":""},{"location":"project-guide/rocket-learn-setup/rocket-learn-setup/#1-install-windows-subsystem-for-linux-wsl-from-powershell","title":"1. Install Windows Subsystem for Linux (WSL) from PowerShell","text":"<pre><code>wsl --install\n</code></pre>"},{"location":"project-guide/rocket-learn-setup/rocket-learn-setup/#2-from-start-search-for-turn-windows-features-on-or-off-and-then-select-windows-subsystem-for-linux","title":"2. From Start, search for Turn Windows features on or off and then select Windows Subsystem for Linux","text":""},{"location":"project-guide/rocket-learn-setup/rocket-learn-setup/#3-once-installed-you-can-run-bash-on-ubuntu-by-typing-bash-from-a-windows-command-prompt-then-you-can-install-recent-stable-versions-of-redis-from-the-official-packagesredisio-apt-repository","title":"3. Once installed, you can run bash on Ubuntu by typing bash from a Windows Command Prompt, then you can install recent stable versions of Redis from the official packages.redis.io APT repository","text":"<pre><code>curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\n\necho \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list\n\nsudo apt-get update\nsudo apt-get install redis\nsudo apt-get install make\nsudo apt-get install gcc\n</code></pre>"},{"location":"project-guide/rocket-learn-setup/rocket-learn-setup/#4-after-installation-start-the-redis-server","title":"4. After installation, start the Redis server","text":"<pre><code>sudo systemctl start redis-server\n</code></pre> <p>Then test if it's running with:</p> <pre><code>$ redis-cli\n$ 127.0.0.1:6379&gt; SET foo bar\nOK\n$ 127.0.0.1:6379&gt; GET foo\n\"bar\"\n</code></pre> <p>You can also check if the server is running in the background with:</p> <pre><code>ps aux | grep redis\n</code></pre> <p>and you should see something like this:</p> <pre><code>redis      26030  3.5  0.2 141256 16216 ?        Ssl  14:43   0:00 /usr/bin/redis-server 127.0.0.1:6379\n</code></pre> <p>To shut down server type:</p> <pre><code>sudo systemctl stop redis-server\n</code></pre>"},{"location":"project-guide/rocket-learn-setup/rocket-learn-setup/#safety-measures","title":"Safety measures","text":"<p>In order to use Redis safely we need to set up it properly and apply appropriate countermeasures. Firstly, we will turn off redis-server autostart on system startup with:</p> <pre><code>sudo update-rc.d redis-server disable\n</code></pre> <p>Secondly, we will block the port that Redis is using with a firewall. By default, that port is 6379 (the port number was displayed next to the IP address while we were testing if the redis-server was running).</p>"},{"location":"project-guide/rocket-learn-setup/rocket-learn-setup/#1-install-ufw","title":"1. Install ufw","text":"<pre><code>sudo apt-get install ufw\n</code></pre>"},{"location":"project-guide/rocket-learn-setup/rocket-learn-setup/#2-allow-local-access-typically-127001-to-redis-and-block-external-access-to-port-6379","title":"2. Allow local access (typically 127.0.0.1) to Redis and block external access to port 6379","text":"<pre><code>sudo ufw deny 6379\nsudo ufw allow from 127.0.0.1 to any port 6379\n</code></pre>"},{"location":"project-guide/rocket-learn-setup/rocket-learn-setup/#3-enable-or-reload-ufw-if-necessary","title":"3. Enable or reload ufw (if necessary)","text":"<p>Enable it with:</p> <pre><code>sudo ufw enable\n</code></pre> <p>If ufw was already enabled, you can reload it to apply the new rules:</p> <pre><code>sudo ufw reload\n</code></pre>"},{"location":"project-guide/rocket-learn-setup/rocket-learn-setup/#4-check-the-firewall-status","title":"4. Check the firewall status","text":"<pre><code>sudo ufw status\n</code></pre> <p>You should see something like this:</p> <pre><code>Status: active\n\nTo                         Action      From\n--                         ------      ----\n6379                       ALLOW       127.0.0.1\n6379                       DENY        Anywhere\n6379 (v6)                  DENY        Anywhere (v6)\n</code></pre> <p>Lastly, we will configure Redis to require a password before allowing any client to execute commands.</p>"},{"location":"project-guide/rocket-learn-setup/rocket-learn-setup/#1-locate-the-redis-configuration-file","title":"1. Locate the Redis Configuration File","text":"<pre><code>sudo find / -name \"redis.conf\" 2&gt;/dev/null\n</code></pre> <p>you should see something like:</p> <pre><code>/etc/redis/redis.conf\n</code></pre>"},{"location":"project-guide/rocket-learn-setup/rocket-learn-setup/#2-edit-the-configuration-file","title":"2. Edit the Configuration File","text":"<pre><code>sudo nano /etc/redis/redis.conf\n</code></pre>"},{"location":"project-guide/rocket-learn-setup/rocket-learn-setup/#3-find-the-following-line-in-the-configuration-file","title":"3. Find the following line in the configuration file","text":"<pre><code># requirepass foobared\n</code></pre>"},{"location":"project-guide/rocket-learn-setup/rocket-learn-setup/#4-uncomment-it-by-removing-the-at-the-beginning-and-change-the-password-foobared-to-your-desired-password","title":"4. Uncomment it (by removing the # at the beginning) and change the password \"foobared\" to your desired password","text":""},{"location":"project-guide/rocket-learn-setup/rocket-learn-setup/#5-save-and-exit-the-file-in-nano-you-can-press-ctrlx-then-y-and-then-enter","title":"5. Save and exit the file (in Nano, you can press Ctrl+X, then Y, and then Enter)","text":""},{"location":"project-guide/rocket-learn-setup/rocket-learn-setup/#6-restart-the-redis-server-to-apply-the-changes","title":"6. Restart the Redis server to apply the changes","text":"<pre><code>sudo systemctl restart redis-server\n</code></pre>"},{"location":"project-guide/rocket-learn-setup/rocket-learn-setup/#7-you-can-test-if-this-works","title":"7. You can test if this works","text":"<pre><code>$ redis-cli\n127.0.0.1:6379&gt; ping\n(error) NOAUTH Authentication required.\n127.0.0.1:6379&gt; AUTH Mystrongpassword\nOK\n127.0.0.1:6379&gt; ping\nPONG\n</code></pre>"},{"location":"project-guide/rocket-learn-setup/rocket-learn-setup/#using-redis-in-the-project","title":"Using Redis in the project","text":"<p>In the project directory alongside this documentation, there also should be 2 files called \"learner-botwheels-example.py\" and \"worker-bothweels-example.py\". Those two examples were taken from the rocket-learn repository and modified, so they could be used as tests to see if the setup works correctly. In order to use them, you need to create a Wandb account and use your credentials in the \"learner_botwheels-example.py\" in lines:</p> <pre><code>wandb.login(key=\"Your wandb API key goes here\")\nlogger = wandb.init(project=\"demo\", entity=\"Your wandb username goes here\")\n</code></pre> <p>Then, in both files you need to insert your Redis password in line:</p> <pre><code>redis = Redis(host='localhost', password=\"Your Redis server password goes here\", port=6379)\n</code></pre> <p>Lastly, if it isn't already inside the project directory, you can clone rocket-learn repo with:</p> <pre><code>git clone https://github.com/Rolv-Arild/rocket-learn.git\n</code></pre> <p>The two files need to be located in the same directory as the rocket-learn.  Start up, in order:</p> <ul> <li>the Redis server</li> <li>the Learner</li> <li>the Workers</li> </ul>"},{"location":"research/a2c-learner-parameters/","title":"Parameters in A2C Learner Configuration","text":""},{"location":"research/a2c-learner-parameters/#1-a2c_batch_size","title":"1. <code>a2c_batch_size</code>","text":"<ul> <li>New Value: <code>5000</code></li> <li>Why Changed: Smaller batches mean the model updates more often and uses less memory.</li> <li>Effect:</li> <li>Faster Learning: The model can adjust quicker to new information.</li> <li>More Stable Training: Reduces the chance of errors during updates.</li> <li>Guidelines:</li> <li>Increase <code>a2c_batch_size</code> When:<ul> <li>You have more memory and computational power.</li> <li>You observe that larger batches lead to more stable gradient estimates.</li> <li>Training is too slow, and larger batches can speed it up without causing memory issues.</li> </ul> </li> <li>Decrease <code>a2c_batch_size</code> When:<ul> <li>Memory constraints prevent using larger batches.</li> <li>Training becomes unstable with larger batch sizes.</li> <li>You need more frequent updates to capture rapid changes in the environment.</li> </ul> </li> </ul>"},{"location":"research/a2c-learner-parameters/#2-ts_per_iteration","title":"2. <code>ts_per_iteration</code>","text":"<ul> <li>New Value: <code>25000</code></li> <li>Why Changed: Fewer timesteps per iteration allow the model to update more frequently.</li> <li>Effect:</li> <li>Quick Adaptation: The model can respond faster to changes in the environment.</li> <li>Better Learning Flow: Keeps the training process smooth and continuous.</li> <li>Guidelines:</li> <li>Increase <code>ts_per_iteration</code> When:<ul> <li>You want the model to collect more experiences before each update, which can improve learning quality.</li> <li>The environment is complex, and more data per iteration helps in better policy updates.</li> </ul> </li> <li>Decrease <code>ts_per_iteration</code> When:<ul> <li>You need more frequent updates to adapt quickly to changing environments.</li> <li>Training is too slow due to the high number of timesteps per iteration.</li> <li>You observe diminishing returns with larger timesteps per iteration.</li> </ul> </li> </ul>"},{"location":"research/a2c-learner-parameters/#3-policy_lr-and-critic_lr","title":"3. <code>policy_lr</code> and <code>critic_lr</code>","text":"<ul> <li>New Values:</li> <li><code>policy_lr</code>: <code>1e-3</code></li> <li><code>critic_lr</code>: <code>1e-3</code></li> <li>Why Changed: Higher learning rates help the model make bigger updates when needed.</li> <li>Effect:</li> <li>Faster Improvement: The policy and value estimates improve more quickly.</li> <li>Effective Updates: Prevents the model from getting stuck without making progress.</li> <li>Guidelines:</li> <li>Increase Learning Rates When:<ul> <li>The model is learning too slowly, and updates are too small to make significant progress.</li> <li>You have a stable training process and can handle larger updates without causing instability.</li> </ul> </li> <li>Decrease Learning Rates When:<ul> <li>The training becomes unstable, with loss values fluctuating wildly or diverging.</li> <li>The model overshoots optimal values, leading to poor performance.</li> <li>Gradient updates are too large, causing the learning process to fail.</li> </ul> </li> </ul>"},{"location":"research/a2c-learner-parameters/#4-a2c_ent_coef","title":"4. <code>a2c_ent_coef</code>","text":"<ul> <li>New Value: <code>0.01</code></li> <li>Why Changed: Increasing entropy helps the model explore more actions.</li> <li>Effect:</li> <li>Better Exploration: The model tries a wider variety of actions, finding better strategies.</li> <li>Prevents Early Stopping: Avoids the model settling on bad solutions too soon.</li> <li>Guidelines:</li> <li>Increase <code>a2c_ent_coef</code> When:<ul> <li>The agent is not exploring enough and is stuck in suboptimal policies.</li> <li>You want to encourage more diversity in the actions taken by the policy.</li> </ul> </li> <li>Decrease <code>a2c_ent_coef</code> When:<ul> <li>The agent is exploring too much and not exploiting learned strategies effectively.</li> <li>Training converges too slowly due to excessive exploration.</li> <li>The environment requires more exploitation of known good actions rather than exploration.</li> </ul> </li> </ul>"},{"location":"research/a2c-learner-parameters/#5-exp_buffer_size","title":"5. <code>exp_buffer_size</code>","text":"<ul> <li>New Value: <code>50000</code></li> <li>Why Changed: Adjusting the experience buffer size ensures that enough experiences are stored for effective learning.</li> <li>Effect:</li> <li>Sufficient Experience Storage: Maintains a diverse set of experiences for training, preventing overfitting to recent experiences.</li> <li>Improved Learning Quality: A larger buffer provides more varied data, enhancing the model's ability to generalize.</li> <li>Guidelines:</li> <li>Increase <code>exp_buffer_size</code> When:<ul> <li>You have ample memory and want to store more experiences for better learning.</li> <li>The environment is highly variable, requiring a larger buffer to capture diverse experiences.</li> </ul> </li> <li>Decrease <code>exp_buffer_size</code> When:<ul> <li>Memory constraints limit the size of the buffer.</li> <li>A smaller buffer suffices for the complexity of the environment and task.</li> </ul> </li> </ul>"},{"location":"research/a2c-learner-parameters/#6-policy_layer_sizes-and-critic_layer_sizes","title":"6. <code>policy_layer_sizes</code> and <code>critic_layer_sizes</code>","text":"<ul> <li>New Values:</li> <li><code>policy_layer_sizes</code>: <code>(256, 128)</code></li> <li><code>critic_layer_sizes</code>: <code>(256, 128)</code></li> <li>Why Changed: Adjusting the size of the neural network layers can help the model better capture complex patterns in the data.</li> <li>Effect:</li> <li>Enhanced Representation: Larger layers can learn more detailed and nuanced strategies.</li> <li>Improved Generalization: Better capacity to generalize from training data to unseen situations.</li> <li>Guidelines:</li> <li>Increase Layer Sizes When:<ul> <li>The model is underfitting, failing to capture complex patterns in the data.</li> <li>You have sufficient computational resources to handle larger networks.</li> <li>The task requires modeling intricate relationships within the data.</li> </ul> </li> <li>Decrease Layer Sizes When:<ul> <li>The model is overfitting, memorizing the training data instead of generalizing.</li> <li>Computational resources are limited, leading to long training times.</li> <li>The task is relatively simple and does not require deep or wide networks.</li> </ul> </li> </ul>"},{"location":"research/a2c-learner-parameters/#7-gae_lambda-and-gae_gamma","title":"7. <code>gae_lambda</code> and <code>gae_gamma</code>","text":"<ul> <li>New Values:</li> <li><code>gae_lambda</code>: <code>0.95</code></li> <li><code>gae_gamma</code>: <code>0.99</code></li> <li>Why Changed: These parameters control the balance between bias and variance in the advantage estimation.</li> <li>Effect:</li> <li>Balanced Advantage Estimation: Proper settings help in reducing the variance without introducing too much bias.</li> <li>Stable Learning: Leads to more reliable updates by providing accurate advantage estimates.</li> <li>Guidelines:</li> <li>Increase <code>gae_lambda</code> When:<ul> <li>You want to reduce bias in advantage estimation, potentially at the cost of increased variance.</li> <li>The environment requires more accurate advantage calculations for effective learning.</li> </ul> </li> <li>Decrease <code>gae_lambda</code> When:<ul> <li>High variance in advantage estimates is causing instability in training.</li> <li>A lower lambda helps in stabilizing the learning process by introducing more bias.</li> </ul> </li> <li>Increase <code>gae_gamma</code> When:<ul> <li>Long-term rewards are more important for the task.</li> <li>The agent needs to consider future rewards more significantly.</li> </ul> </li> <li>Decrease <code>gae_gamma</code> When:<ul> <li>Short-term rewards are more critical.</li> <li>The environment rewards immediate actions more than future ones.</li> </ul> </li> </ul>"},{"location":"research/a2c-learner-parameters/#summary-of-key-changes","title":"Summary of Key Changes","text":"<pre><code>learner = Learner(\n    build_rocketsim_env,\n    n_proc=n_proc,\n    min_inference_size=min_inference_size,\n    metrics_logger=metrics_logger,\n    a2c_batch_size=5000,\n    ts_per_iteration=25000,\n    exp_buffer_size=50000,\n    policy_layer_sizes=(256, 128),\n    critic_layer_sizes=(256, 128),\n    log_to_wandb=False,\n    render=True,\n    add_unix_timestamp=False,\n    render_delay=0.02,\n    continuous_var_range=(0.1, 1.0),\n    a2c_ent_coef=0.01,\n    gae_lambda=0.95,\n    gae_gamma=0.99,\n    policy_lr=1e-3,\n    critic_lr=1e-3,\n)\n</code></pre>"},{"location":"research/implement-movement-testing-script-for-bot-actions/","title":"Rocket League-Style Bot Testing Framework","text":"<p>Code implements a mock testing framework for a bot in the context of a Rocket League-style simulation. It tests various bot actions such as moving forward, using boost, turning, and stopping. Here is an overview of the key components:</p>"},{"location":"research/implement-movement-testing-script-for-bot-actions/#key-imports","title":"Key Imports","text":"<ul> <li>SimpleControllerState: Imported from <code>rlbot.agents.base_agent</code>. This class represents the controller's state (e.g., throttle, steer, boost).</li> <li>MagicMock: From <code>unittest.mock</code>. It\u2019s used to mock the game's data structures, enabling isolated unit testing without actual game data.</li> <li>time: Used for handling time intervals and simulating the passage of time in tests.</li> </ul>"},{"location":"research/implement-movement-testing-script-for-bot-actions/#mock-classes","title":"Mock Classes","text":"<ul> <li>MockPhysics: Mocks the physics state of a car. It has location and velocity, both mocked with <code>MagicMock</code>. The position starts at the origin (0, 0, 0), and the velocity is initially zero.</li> <li>MockCar: Represents a car in the game, using <code>MockPhysics</code> to simulate its physical state.</li> <li>MockGameTickPacket: Simulates the data structure that provides information about the game state. It contains a list of game cars (<code>game_cars</code>), in this case, only one <code>MockCar</code>.</li> <li>MockLogger: A mocked logger that captures logs for testing. It stores logs in a list so they can be checked after tests.</li> </ul>"},{"location":"research/implement-movement-testing-script-for-bot-actions/#movementtestbot-class","title":"MovementTestBot Class","text":"<p>This class simulates a bot that performs a series of tests on its movement capabilities.</p>"},{"location":"research/implement-movement-testing-script-for-bot-actions/#attributes","title":"Attributes","text":"<ul> <li>index, team, name: Basic information about the bot.</li> <li>logger: An instance of <code>MockLogger</code>, used to log information during the tests.</li> <li>controller_state: An instance of <code>SimpleControllerState</code> that controls the bot's movement commands (throttle, steer, boost).</li> <li>test_duration: Duration of each individual test, set to 1 second.</li> <li>log: A list to store test results.</li> <li>tests: A list of movement test methods (<code>test_move_forward</code>, <code>test_use_boost</code>, etc.).</li> <li>current_test_index: Tracks which test is currently running.</li> <li>test_start_time and test_logged: Variables to handle the timing and logging of tests.</li> </ul>"},{"location":"research/implement-movement-testing-script-for-bot-actions/#methods","title":"Methods","text":"<ul> <li>get_output(packet): Called every frame to update the bot's behavior. It runs the current test, checks if the test duration has passed, logs the results, and moves to the next test.</li> <li>test_move_forward(packet): Simulates moving forward by increasing the car's x-coordinate.</li> <li>test_use_boost(packet): Simulates using boost, increasing both the x- and z-coordinates.</li> <li>test_turn_left(packet): Simulates turning left by decreasing the y-coordinate.</li> <li>test_turn_right(packet): Simulates turning right by increasing the y-coordinate.</li> <li>test_stop(packet): Stops the bot by setting throttle and boost to 0, and velocity to 0.</li> <li>log_test_result(packet): Logs the results of the current test, including the car's position and velocity.</li> </ul>"},{"location":"research/implement-movement-testing-script-for-bot-actions/#simplecontrollerstate-class","title":"SimpleControllerState Class","text":"<p>This mock version of the <code>SimpleControllerState</code> mimics the behavior of the controller, with attributes for throttle, steer, and boost. It is manipulated by the bot during tests.</p>"},{"location":"research/implement-movement-testing-script-for-bot-actions/#run_test-function","title":"run_test() Function","text":"<p>This function simulates a test environment:</p> <ol> <li>Creates a <code>MockLogger</code> to capture logs.</li> <li>Instantiates <code>MovementTestBot</code> with test parameters.</li> <li>Simulates game frames by calling <code>get_output(packet)</code> 300 times (with a 100ms delay per frame) to allow all tests to run.</li> <li>Prints the logged test results after the test loop completes.</li> </ol>"},{"location":"research/implement-movement-testing-script-for-bot-actions/#summary-of-bot-tests","title":"Summary of Bot Tests","text":"<ul> <li>Test 1: Move Forward \u2014 The bot moves forward by increasing its x-coordinate.</li> <li>Test 2: Use Boost \u2014 The bot moves faster using boost, increasing both x and z.</li> <li>Test 3: Turn Left \u2014 The bot steers left, decreasing its y-coordinate.</li> <li>Test 4: Turn Right \u2014 The bot steers right, increasing its y-coordinate.</li> <li>Test 5: Stop \u2014 The bot stops by setting velocity to zero and turning off throttle and boost.</li> </ul>"},{"location":"research/implement-movement-testing-script-for-bot-actions/#objective","title":"Objective","text":"<p>The main objective of this code is to provide a unit test framework for the bot\u2019s movement using mocked data structures for game physics and a simulated environment.</p>"},{"location":"research/match-against-ai-bots/","title":"Unexpected problems and delays","text":"<p>@Garikmal and I (@Oruniarz) were assigned issue #39. During our work on it, we discovered that adding a bot controlled by some AI isn't as easy as we thought and also isn't exactly supported by RLGym package. This will prevent this issue from being completed until a decision is made on how this problem should be addressed.  There are some packages that allow for adding some already existent bots as opponents in the process of training our own such as: </p> <ul> <li>rlgym-ppo,</li> <li>rocket-learn.</li> </ul> <p>The first one uses a simulator of Rocket League RocketSim and RLGym Wrapper rocket-league-gym-sim and would require to move from the main game and RLGym library. The second one would require some additional setup and modifications of the main project. It would also mean that all contributors will need to learn how to use this library.</p>"},{"location":"research/match-against-ai-bots/#some-useful-links-found-while-making-this-research","title":"Some useful links found while making this research","text":"<ul> <li>RLGym-PPO-Guide - useful guide that could help with creating the bot</li> </ul>"},{"location":"research/playing-in-online-games/","title":"Use of Bots in Online Gaming (Rocket League)","text":""},{"location":"research/playing-in-online-games/#rlbot-framework-online-play-restrictions","title":"RLBot Framework: Online Play Restrictions","text":"<p>According to the official documentation, RLBot cannot be used in online Rocket League matches. The framework works by disabling online play when activated through a special setting. Attempts to circumvent this restriction may result in a ban, as the developers (Psyonix) actively discourage using bots in ways that might be perceived as malicious (e.g., farming, leveling accounts). This restriction makes it impossible to use RLBot for online matches with random players, as it conflicts with Psyonix's terms of service. Information found in the RLBot FAQ.</p>"},{"location":"research/playing-in-online-games/#impact-on-our-project","title":"Impact on Our Project","text":"<p>Given these restrictions, we cannot test our bots online against real players. Instead, we will adapt our approach by running matches locally, either:</p> <ul> <li>Against team members participating in the project</li> <li>Against members of the RLBot community or friends who can participate in LAN or VLAN settings</li> </ul> <p>This setup will allow us to fulfill the project goal of competing against silver-level players while adhering to the restrictions placed by Psyonix. Testing in a controlled, local environment ensures compliance and protects the integrity of the project.</p>"},{"location":"research/playing-in-online-games/#alternatives-lanvlan-matches","title":"Alternatives: LAN/VLAN Matches","text":"<p>While RLBot cannot be used for online play, it is possible to set up local matches using a LAN or VLAN connection, allowing us to play with others, but only within the same network. This ensures that we still have access to human opponents, even though cross-platform play is not supported in these setups.</p>"},{"location":"research/playing-in-online-games/#conclusion","title":"Conclusion","text":"<p>Due to the limitations of the RLBot framework and the anti-cheat policies of Rocket League, we will conduct our bot matches locally. We plan to involve project members, community players, or friends to provide the necessary competition. This approach ensures compliance with all regulations while still allowing us to achieve the core goals of our project.</p>"},{"location":"research/research-about-different-reward-functions/","title":"Broad Overview: Designing Reward Functions with Insights from Necto Bot","text":""},{"location":"research/research-about-different-reward-functions/#task-context","title":"Task Context","text":"<p>Designing effective reward functions is a core challenge in reinforcement learning, particularly in a complex multi-agent environment like Rocket League. The reward function plays a critical role in guiding agents (bots) to learn behaviors that align with the game's objectives, such as scoring goals, defending, and collaborating with teammates. Bots like Necto provide valuable insights into reward design due to their sophisticated strategies and adaptive gameplay.</p> <p>This task involves:</p> <ol> <li>Researching reward function implementations in bots like Necto and others.</li> <li>Analyzing these implementations to identify strengths, gaps, and potential improvements.</li> <li>Developing novel reward function ideas based on the analysis and adapting them to suit specific training objectives.</li> </ol>"},{"location":"research/research-about-different-reward-functions/#step-1-research-reward-functions","title":"Step 1: Research Reward Functions","text":""},{"location":"research/research-about-different-reward-functions/#insights-from-necto-bot","title":"Insights from Necto Bot","text":"<p>Necto is a highly advanced bot trained using reinforcement learning techniques. Its reward function reflects a mix of dense, sparse, and task-specific strategies:</p> <ul> <li>Dense Rewards: Encourages continuous improvement by rewarding small, incremental actions (e.g., ball touches, aligning with the ball's trajectory, or accelerating the ball).</li> <li>Sparse Rewards: Rewards only critical events, such as scoring goals or making saves.</li> <li>Shaped Rewards: Combines both dense and sparse rewards to promote specific behaviors like defending, attacking, or recovering boost.</li> </ul>"},{"location":"research/research-about-different-reward-functions/#other-bots-for-comparison","title":"Other Bots for Comparison","text":"<ul> <li>Nexto: Focuses on balanced team play and rewards for proximity to the ball, strategic positioning, and efficient boost usage.</li> <li>RLBot Framework: Includes simpler, task-oriented rewards for specific events like demos or clearances.</li> <li>Rocket-League-AI: Uses dense rewards for ball handling and positioning but lacks nuanced team dynamics.</li> </ul>"},{"location":"research/research-about-different-reward-functions/#step-2-broader-design-strategies","title":"Step 2: Broader Design Strategies","text":""},{"location":"research/research-about-different-reward-functions/#common-approaches-in-reward-design","title":"Common Approaches in Reward Design","text":"<ol> <li> <p>State-Based Rewards    Rewards based on the bot's current state (e.g., position, velocity) relative to the game elements (ball, goal).    Encourages maintaining advantageous positions.</p> </li> <li> <p>Event-Based Rewards    Rewards tied to game events (e.g., scoring, saving, or assisting a goal).    Ensures the bot learns key objectives.</p> </li> <li> <p>Dynamic Contextual Rewards    Adjusts rewards based on game state (e.g., time left, score difference).    Helps bots adopt different strategies, such as defensive play when leading or aggressive play when trailing.</p> </li> <li> <p>Shaping Rewards    Provides intermediate rewards for progress toward a goal (e.g., dribbling the ball closer to the opponent's net).    Useful for guiding bots in complex environments.</p> </li> </ol>"},{"location":"research/research-about-different-reward-functions/#step-3-analyze-nectos-reward-function","title":"Step 3: Analyze Necto\u2019s Reward Function","text":""},{"location":"research/research-about-different-reward-functions/#strengths","title":"Strengths","text":"<ul> <li>Comprehensive: Covers a wide range of behaviors, from individual skills (like ball control) to team strategies (like positioning).</li> <li>Adaptive: Uses game-state information to adjust rewards dynamically, encouraging strategic decision-making.</li> <li>Nuanced: Incorporates details like ball height, wall positioning, and flip resets to guide advanced play.</li> </ul>"},{"location":"research/research-about-different-reward-functions/#weaknesses","title":"Weaknesses","text":"<ul> <li>Complexity: High computational cost due to dense reward calculations.</li> <li>Potential Exploits: Bots might \"farm\" rewards for specific actions, such as unnecessary ball touches or inefficient positioning.</li> <li>Limited Exploration: Reward functions tied to predefined behaviors may restrict bots from discovering innovative strategies.</li> </ul>"},{"location":"research/research-about-different-reward-functions/#step-4-generate-new-ideas","title":"Step 4: Generate New Ideas","text":"<p>Based on insights from Necto and reinforcement learning literature, here are broader reward function ideas:</p> <ol> <li>Dynamic Role-Based Rewards </li> <li>Concept: Introduce rewards that adapt based on the bot\u2019s role (e.g., attacker, defender, or midfielder).  </li> <li>Implementation:<ul> <li>Reward defenders for staying near the goal line and clearing the ball.</li> <li>Reward attackers for positioning near the opponent's goal and taking shots.  </li> </ul> </li> <li> <p>Benefit: Encourages specialized behaviors within a team.</p> </li> <li> <p>Game-State Aware Rewards </p> </li> <li>Concept: Adjust rewards based on game context, such as score difference, remaining time, or ball position.  </li> <li>Implementation:<ul> <li>When leading: Reward possession and defensive actions.</li> <li>When trailing: Reward aggressive plays and high-risk strategies.  </li> </ul> </li> <li> <p>Benefit: Improves situational awareness and strategy adaptation.</p> </li> <li> <p>Opponent-Focused Rewards </p> </li> <li>Concept: Encourage actions that disrupt opponents, such as blocking shots, forcing bad touches, or demos.  </li> <li>Implementation:<ul> <li>Reward for intercepting passes or cutting off opponents' angles.</li> <li>Penalize over-aggression if it leaves the bot out of position.  </li> </ul> </li> <li> <p>Benefit: Promotes intelligent pressure and disrupts opponent strategies.</p> </li> <li> <p>Exploration Rewards </p> </li> <li>Concept: Reward exploration of new strategies, such as creative passes, wall play, or aerial maneuvers.  </li> <li>Implementation:<ul> <li>Provide rewards for attempting advanced techniques like flip resets or ceiling shots.</li> <li>Include diminishing returns to prevent farming.  </li> </ul> </li> <li> <p>Benefit: Encourages bots to learn and innovate.</p> </li> <li> <p>Resource Efficiency Rewards </p> </li> <li>Concept: Reward bots for efficient use of boost and energy.  </li> <li>Implementation:<ul> <li>Penalize wasteful boost usage when the bot is not actively contributing to the play.</li> <li>Reward conservation during defensive rotations or strategic waiting.  </li> </ul> </li> <li>Benefit: Promotes sustainable play and improves long-term decision-making.</li> </ol>"},{"location":"research/research-about-different-reward-functions/#broad-applications","title":"Broad Applications","text":"<p>These ideas can be tailored to:</p> <ul> <li> <p>Training Scenarios:   Focus on specific skills like aerials or dribbling.   Encourage strategic team-based play.</p> </li> <li> <p>Competitive Bots:   Optimize for high-level play by balancing individual performance with team dynamics.</p> </li> <li> <p>Reinforcement Learning Research:   Explore the effects of novel reward structures on agent learning and behavior.</p> </li> </ul> <p>By analyzing and building upon Necto's reward function, these strategies provide a framework for designing advanced, adaptable, and innovative bots capable of handling Rocket League's complex dynamics.</p>"},{"location":"research/research-and-documentation-of-algorithm-categories-for-rocket-league-bot/","title":"Research and Documentation of Algorithm Categories for Rocket League Bot","text":""},{"location":"research/research-and-documentation-of-algorithm-categories-for-rocket-league-bot/#q-learning-anna-ivanytska","title":"Q-Learning - Anna Ivanytska","text":""},{"location":"research/research-and-documentation-of-algorithm-categories-for-rocket-league-bot/#materials","title":"Materials","text":"<ul> <li>Q-Learning Algorithm: From Explanation to Implementation | by Amrani Amine | Towards Data Science </li> <li>An Introduction to Q-Learning: A Tutorial For Beginners | DataCamp </li> <li>Reinforcement Learning Explained Visually (Part 4): Q Learning, step-by-step | by Ketan Doshi | Towards Data Science</li> </ul>"},{"location":"research/research-and-documentation-of-algorithm-categories-for-rocket-league-bot/#what-is-q-learning","title":"What is Q-learning?","text":"<p>Q-learning is a model-free, value-based, off-policy algorithm that finds the best series of actions based on the agent's current state. The \u201cQ\u201d stands for quality, representing how valuable an action is in maximizing future rewards.</p>"},{"location":"research/research-and-documentation-of-algorithm-categories-for-rocket-league-bot/#examples-of-uses","title":"Examples of uses","text":"<p>None found.</p>"},{"location":"research/research-and-documentation-of-algorithm-categories-for-rocket-league-bot/#why-does-it-have-limited-popularity-in-developing-bots-for-rocket-league","title":"Why does it have limited popularity in developing bots for Rocket League?","text":"<ol> <li>High Dimensionality and Complexity: Rocket League is a fast-paced game with a complex environment, involving variables such as player positions, ball trajectory, and physics interactions. This vast state space makes it difficult for Q-learning algorithms to converge to optimal policies, as they require substantial data to learn effectively.</li> <li>Inconsistent Human Behavior: Human players display a wide range of strategies and behaviors, which complicates learning. Since no single optimal action exists in many scenarios, Q-learning struggles to generalize from diverse actions, leading to inconsistent learning outcomes.</li> <li>Compounding Errors: A single incorrect action can lead to unfamiliar states, making learning more difficult. This issue is magnified in Rocket League, where quick decision-making is crucial.</li> <li>Resource Intensity: Developing a competent Q-learning bot requires significant computational resources and time due to the need for numerous simulations.</li> </ol>"},{"location":"research/research-and-documentation-of-algorithm-categories-for-rocket-league-bot/#conclusion","title":"Conclusion","text":"<p>Due to these challenges, Rocket League bot developers prefer more advanced machine learning techniques, such as deep reinforcement learning or imitation learning, which can handle the game's complexity more effectively.</p>"},{"location":"research/research-and-documentation-of-algorithm-categories-for-rocket-league-bot/#policy-optimization-igor-malkovskiy","title":"Policy Optimization - Igor Malkovskiy","text":""},{"location":"research/research-and-documentation-of-algorithm-categories-for-rocket-league-bot/#materialss","title":"Materialss","text":"<ul> <li>Policy Gradient Algorithms Explained by Arthur Juliani | Towards Data Science </li> <li>Parts 2: Kinds of RL Algorithms </li> <li>Part 3: Intro to Policy Optimization </li> <li>Proximal Policy Optimization</li> </ul>"},{"location":"research/research-and-documentation-of-algorithm-categories-for-rocket-league-bot/#what-is-policy-optimization","title":"What is Policy Optimization?","text":"<p>Policy Optimization is a reinforcement learning method that improves the agent\u2019s behavior policy by directly optimizing its parameters. Unlike value-based methods like Q-learning, which optimize the value function, Policy Optimization focuses on continuous improvement of the policy itself.</p>"},{"location":"research/research-and-documentation-of-algorithm-categories-for-rocket-league-bot/#examples-of-usess","title":"Examples of usess","text":"<ul> <li>Building a reinforcement learning agent that can play Rocket League</li> </ul>"},{"location":"research/research-and-documentation-of-algorithm-categories-for-rocket-league-bot/#why-policy-optimization-isnt-popular-for-developing-bots-in-rocket-league","title":"Why Policy Optimization Isn't Popular for Developing Bots in Rocket League","text":"<ol> <li>High Computational Complexity: Policy optimization algorithms, such as Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO), require significant computational resources. Rocket League\u2019s complex physical interactions make training and optimizing policies resource-intensive, as numerous simulations are necessary.</li> <li>Sensitivity to Hyperparameters: Policy optimization algorithms are highly sensitive to hyperparameters like learning rate and regularization coefficients. Incorrect tuning can lead to unstable or non-functional learning, complicating the application in complex environments like Rocket League.</li> <li>Generalization Problems: Rocket League presents a wide range of fast-changing scenarios. Policy optimization algorithms often struggle to generalize in such situations, particularly when continuous control is required.</li> <li>Complexity of Reward Shaping: Effective reward design is crucial in policy optimization, and in Rocket League, rewards are challenging to formulate due to the combination of short-term goals (e.g., hitting the ball) and long-term strategies (e.g., ball possession and positioning).</li> </ol>"},{"location":"research/research-and-documentation-of-algorithm-categories-for-rocket-league-bot/#why-policy-optimization-could-be-useful-for-developing-bots-in-rocket-league","title":"Why Policy Optimization Could Be Useful for Developing Bots in Rocket League","text":"<ol> <li>Effectiveness in Continuous Action Spaces: Policy optimization excels in environments with continuous action spaces, making it potentially suitable for games like Rocket League that require precise control over the car and the ball.</li> <li>Resilience to Noise and Stochastic Scenarios: Policy optimization handles noise and stochastic scenarios better than value-based methods. This is particularly advantageous in Rocket League, where the game's dynamics can change rapidly due to unexpected ball movements or opponent strategies.</li> </ol>"},{"location":"research/research-and-documentation-of-algorithm-categories-for-rocket-league-bot/#conclusionn","title":"Conclusionn","text":"<p>Although Policy Optimization offers theoretical advantages in environments with continuous action spaces like Rocket League, its use is limited in practice due to high computational demands, generalization challenges, and sensitivity to hyperparameters. Developers often prefer simpler and less resource-intensive approaches, such as Deep Q-Learning or hybrid algorithms.</p>"},{"location":"research/rocket-league-bot-setup/","title":"Rocket League AI Agent Setup","text":""},{"location":"research/rocket-league-bot-setup/#objective","title":"Objective","text":"<p>Set up the environment necessary for integrating an AI agent with Rocket League. We will walk through installing the game, relevant mods, and setting up the development environment.</p>"},{"location":"research/rocket-league-bot-setup/#1-requirements","title":"1. Requirements","text":"<p>Before we begin setting up the environment, ensure that you have the following:</p> <ul> <li>A Windows 10 PC: The setup is tested and supported on Windows 10.</li> <li>Rocket League: Both Steam and Epic versions of Rocket League are supported.</li> <li>Bakkesmod: A popular mod for Rocket League that allows integration of Python scripts and other plugins with the game.</li> <li>RLGym plugin for Bakkesmod: Essential for interfacing with the game. If you install RLGym via pip, this will be installed automatically.</li> <li>Python: Version between 3.7 and 3.9 is required (version 3.10 is not supported). We recommend using the stable version 3.8.19.</li> </ul>"},{"location":"research/rocket-league-bot-setup/#detailed-steps","title":"Detailed Steps","text":""},{"location":"research/rocket-league-bot-setup/#11-install-rocket-league","title":"1.1 Install Rocket League","text":"<ul> <li>Install Rocket League from either Steam or Epic Games Store. This will serve as the base game that the AI agent will interact with.</li> </ul>"},{"location":"research/rocket-league-bot-setup/#12-install-bakkesmod","title":"1.2 Install Bakkesmod","text":"<ul> <li>Bakkesmod is a tool that enables external modding capabilities, including Python script integration with Rocket League. It\u2019s a critical mod for controlling aspects of the game programmatically.</li> <li>Download and install Bakkesmod from the official Bakkesmod website.</li> </ul>"},{"location":"research/rocket-league-bot-setup/#13-install-rlgym-plugin","title":"1.3 Install RLGym Plugin","text":"<ul> <li>RLGym is a framework that allows you to develop and train AI agents for Rocket League. It interacts with Bakkesmod and provides a streamlined interface for AI development.</li> <li>If you are installing RLGym via pip, the necessary plugin for Bakkesmod will be installed automatically. Otherwise, you can manually install the plugin from the RLGym GitHub repository.</li> </ul>"},{"location":"research/rocket-league-bot-setup/#14-install-python","title":"1.4 Install Python","text":"<ul> <li>Download and install Python, ensuring you have a version between 3.7 and 3.9. The recommended version for this setup is Python 3.8.19.</li> <li>You can download Python from the official Python website.</li> <li>During installation, ensure that you check the box \"Add Python to PATH\" for ease of use in command-line operations.</li> </ul>"},{"location":"research/rocket-league-bot-setup/#2-configuring-rocket-league-and-bakkesmod","title":"2. Configuring Rocket League and Bakkesmod","text":"<p>Once the installation is complete, follow these steps to launch the game and configure Bakkesmod:</p>"},{"location":"research/rocket-league-bot-setup/#21-launch-rocket-league","title":"2.1 Launch Rocket League","text":"<ul> <li>Open the Epic Games Launcher and launch Rocket League from your library.</li> </ul>"},{"location":"research/rocket-league-bot-setup/#22-configure-bakkesmod","title":"2.2 Configure Bakkesmod","text":"<ul> <li>After launching Rocket League, open Bakkesmod by pressing the default hotkey F2.</li> <li>Navigate to the \"Python Scripts\" tab. This is where you will manage Python script integration with the game.</li> </ul>"},{"location":"research/rocket-league-bot-setup/#23-plugin-configuration","title":"2.3 Plugin Configuration","text":"<ul> <li>Once in the Bakkesmod window, go to the \"Plugins\" section.</li> <li>Open the Plugin Manager and search for the Rocket League Gym plugin.</li> <li>Check the box next to Rocket League Gym to enable the plugin.</li> </ul> <p>At this stage, you should have Rocket League and Bakkesmod running, with the necessary plugins configured. The game is now ready for interaction with AI scripts.</p>"},{"location":"research/rocket-league-bot-setup/#3-setting-up-the-python-environment","title":"3. Setting up the Python Environment","text":"<p>To ensure smooth interaction between Python and Rocket League, we recommend using a virtual environment like Conda. This will help manage dependencies and prevent conflicts with other projects.</p>"},{"location":"research/rocket-league-bot-setup/#31-install-python-3819","title":"3.1 Install Python 3.8.19","text":"<ul> <li>First, download and install Python 3.8.19 from the official Python website. This is the most stable version for our setup.</li> </ul>"},{"location":"research/rocket-league-bot-setup/#32-create-a-virtual-environment-using-conda","title":"3.2 Create a Virtual Environment using Conda","text":"<ul> <li>If you have Conda installed, create a new virtual environment by running the following commands in your terminal:</li> </ul> <p>```bash   conda create --name rocketleague_env python=3.8.19   conda activate rocketleague_env</p> <p>This will create and activate a new virtual environment with Python 3.8.19.</p>"},{"location":"research/rocket-league-bot-setup/#33-install-required-packages","title":"3.3 Install Required Packages","text":"<ul> <li> <p>Once the environment is activated, install the necessary packages by running the following commands:</p> </li> <li> <p>Install pywin32 version 228, which is required for certain Windows-specific operations:</p> <p><code>bash  pip install pywin32==228</code></p> </li> <li> <p>Install RLGym, the key package that interfaces with Rocket League:</p> <p><code>bash  pip install rlgym</code></p> </li> </ul> <p>With these steps completed, your Python environment is fully configured and ready for development with Rocket League.</p>"},{"location":"research/rocket-league-bot-setup/#4-example-code-for-agent-interaction","title":"4. Example Code for Agent Interaction","text":"<p>In this section, we'll walk through the basic structure of a Python bot for Rocket League. The setup consists of two main files:</p> <ul> <li><code>main.py</code>: This file handles the creation of the agent object and determines whether to train the agent or simply run it.</li> <li><code>Agent.py</code>: This file defines the <code>SimpleAgent</code> class, which contains the logic for training and running the agent.</li> </ul>"},{"location":"research/rocket-league-bot-setup/#41-mainpy","title":"4.1 <code>main.py</code>","text":"<p>Here is the content of the <code>main.py</code> file, which initializes the agent and provides an option to train or run it:</p> <p>```python     from Agent import SimpleAgent</p> <pre><code>if __name__ == \"__main__\":\n    agent = SimpleAgent(real_time=True)\n    mode = input(\"Enter 'train' to train or 'run' to run the agent: \").strip().lower()\n\n    if mode == \"train\":\n        agent.train(timesteps=10000)\n    elif mode == \"run\":\n        agent.run_infinite()\n</code></pre> <p>```</p>"},{"location":"research/rocket-league-bot-setup/#42-agentpy","title":"4.2 <code>Agent.py</code>","text":"<p>The <code>SimpleAgent</code> class is responsible for creating and managing the agent. It includes methods for training, loading models, and running the agent in the Rocket League environment. Below is the key implementation of the agent class:</p> <pre><code>import os\nimport rlgym\nfrom stable_baselines3 import PPO\nimport time\nfrom rlgym.utils.reward_functions.common_rewards import VelocityBallToGoalReward\nfrom rlgym.utils.terminal_conditions.common_conditions import TimeoutCondition\nfrom rlgym.utils.obs_builders import DefaultObs\nfrom rlgym.utils.action_parsers import ContinuousAction\n\ndef make_env():\n    return rlgym.make(\n        obs_builder=DefaultObs(),\n        action_parser=ContinuousAction(),\n        reward_fn=VelocityBallToGoalReward(),\n        terminal_conditions=[TimeoutCondition(225)]\n    )\n\nclass SimpleAgent:\n    def __init__(self, model_path=\"ppo_rlgym_model.zip\", real_time=True):\n        self.model_path = model_path\n        self.real_time = real_time\n        self.env = make_env()\n        self.model = None\n\n    def train(self, timesteps=10000):\n        if os.path.exists(self.model_path):\n            print(f\"Loading existing model from {self.model_path}\")\n            self.model = PPO.load(self.model_path, env=self.env)\n        else:\n            print(\"No existing model found, training from scratch.\")\n            self.model = PPO(\"MlpPolicy\", self.env, verbose=1)\n\n        self.model.learn(total_timesteps=timesteps)\n        self.model.save(self.model_path)\n        print(f\"Model saved to {self.model_path}\")\n\n    def load_model(self):\n        self.model = PPO.load(self.model_path)\n        print(f\"Model loaded from {self.model_path}\")\n\n    def run(self, episodes=100):\n        if self.model is None:\n            self.load_model()\n\n        for _ in range(episodes):\n            obs = self.env.reset()\n            done = False\n            while not done:\n                start_time = time.time()\n                action, _ = self.model.predict(obs)\n                next_obs, reward, done, gameinfo = self.env.step(action)\n                obs = next_obs\n\n                if self.real_time:\n                    elapsed = time.time() - start_time\n                    time.sleep(max(0, 1/60.0 - elapsed))  # Adjust to match 60 FPS\n\n    def run_infinite(self):\n        while True:\n            self.run(episodes=1)\n</code></pre>"},{"location":"research/rocket-league-bot-setup/#43-detailed-explanation-of-the-code","title":"4.3 Detailed Explanation of the Code","text":"<ol> <li><code>make_env()</code>:</li> <li>This function creates the Rocket League environment that the agent interacts with. It uses the following key components:<ul> <li><code>DefaultObs()</code>: Defines how observations are generated for the agent. This includes the positions, velocities, and other relevant data of the ball and cars.</li> <li><code>ContinuousAction()</code>: Defines the action space as continuous, allowing the agent to make precise adjustments to the car\u2019s movement.</li> <li><code>VelocityBallToGoalReward()</code>: A reward function that encourages the agent to push the ball towards the goal. The faster the ball moves towards the goal, the higher the reward.</li> <li><code>TimeoutCondition(225)</code>: Ends the episode after 225 game ticks (approximately 3.75 minutes). This serves as a terminal condition to ensure that the agent doesn\u2019t train indefinitely in a single episode.</li> </ul> </li> </ol> <p>When the <code>make_env()</code> function is called, it sets up the environment with these components, which are critical for defining how the agent perceives the game and what actions it can take.</p> <ol> <li><code>SimpleAgent.__init__()</code>:</li> <li> <p>The constructor (<code>__init__</code>) initializes the <code>SimpleAgent</code> object. Here\u2019s what happens step by step:</p> <ul> <li><code>model_path=\"ppo_rlgym_model.zip\"</code>: The path where the trained model will be saved or loaded from. If the model exists, it will be loaded from this path when the agent is run.</li> <li><code>real_time=True</code>: A flag that determines whether the agent will run in real-time, syncing actions with a 60 FPS frame rate.</li> <li><code>self.env = make_env()</code>: Calls the <code>make_env()</code> function to set up the Rocket League environment.</li> <li><code>self.model = None</code>: The model is initialized as <code>None</code>, and will later be set when the agent trains or loads a pre-existing model.</li> </ul> </li> <li> <p><code>train(timesteps=10000)</code>:</p> </li> <li> <p>This method is used to train the agent. Here\u2019s what happens in sequence:</p> <ul> <li>Check for existing model: The code first checks if a model already exists at <code>self.model_path</code>. If so, it loads the existing model using <code>PPO.load(self.model_path, env=self.env)</code>. This allows for continued training from where the previous model left off.</li> <li>Training from scratch: If no model is found, the agent will start training from scratch using the Proximal Policy Optimization (PPO) algorithm. It initializes a new model with the policy type <code>MlpPolicy</code> (a multi-layer perceptron) and the environment created earlier.</li> <li>Learning: The agent is trained for a specified number of timesteps (10,000 in this case). During this process, the agent interacts with the environment and learns how to maximize the reward (in this case, pushing the ball towards the goal).</li> <li>Saving the model: After training, the model is saved to the specified path (<code>self.model_path</code>) for future use. This allows the agent to load the trained model and continue improving or directly use it later.</li> </ul> </li> <li> <p><code>load_model()</code>:</p> </li> <li> <p>This method is used to load a pre-trained model from <code>self.model_path</code>. Here\u2019s what happens:</p> <ul> <li>Loading the model: The model is loaded from the saved file using <code>PPO.load(self.model_path)</code>.</li> <li>Model ready for use: After loading, the agent can use the model to predict actions and interact with the environment without further training.</li> </ul> </li> <li> <p><code>run(episodes=100)</code>:</p> </li> <li> <p>This method runs the agent in the Rocket League environment for a specified number of episodes (100 by default). Here\u2019s what happens step by step:</p> <ul> <li>Model loading: If the model hasn\u2019t been loaded yet, <code>self.load_model()</code> is called to load the pre-trained model.</li> <li>Environment reset: At the beginning of each episode, the environment is reset using <code>obs = self.env.reset()</code>. This provides the agent with a fresh start for each episode.</li> <li>Agent action loop: The agent repeatedly takes actions in the environment until the episode is over:</li> <li>Action prediction: The agent predicts the next action based on the current observation (<code>action, _ = self.model.predict(obs)</code>).</li> <li>Step in environment: The agent applies the action and steps forward in the environment (<code>next_obs, reward, done, gameinfo = self.env.step(action)</code>). This returns the next observation (<code>next_obs</code>), the reward for the action, and whether the episode is done (<code>done</code>).</li> <li>Update observation: The observation is updated for the next step (<code>obs = next_obs</code>).</li> <li>Real-time adjustment (if <code>real_time=True</code>): The loop is adjusted to run at 60 FPS by calculating the time it took to predict and step, then sleeping for the remaining time in the frame (<code>time.sleep(max(0, 1/60.0 - elapsed))</code>).</li> <li>Episode completion: Once the episode is done, the loop moves to the next episode and resets the environment.</li> </ul> </li> <li> <p><code>run_infinite()</code>:</p> </li> <li>This method continuously runs the agent without stopping. It calls <code>run(episodes=1)</code> in an infinite loop:<ul> <li>Infinite loop: The agent keeps running one episode at a time in an infinite loop. This is useful for long-term testing or for scenarios where the agent needs to run indefinitely.</li> <li>Real-time interaction: If <code>real_time=True</code>, the agent\u2019s actions will be synced with 60 FPS, simulating real-time gameplay as closely as possible.</li> </ul> </li> </ol>"},{"location":"research/rl-algorithms/A2C/","title":"A2C Algorithm","text":""},{"location":"research/rl-algorithms/A2C/#about","title":"About","text":"<p>A2C (Advantage Actor-Critic) is a reinforcement learning algorithm that builds on the core principles of the Actor-Critic framework. It is a simplified and synchronized version of A3C (Asynchronous Advantage Actor-Critic), designed to improve stability and performance by using synchronized updates. A2C is widely used in environments with large, continuous action spaces and complex decision-making tasks. The key components of A2C include:</p> <ul> <li> <p>Actor Network: This deep neural network maps observations from the environment to a probability distribution over actions. It is trained using policy gradient methods to maximize the expected future rewards by adjusting the action probabilities.</p> </li> <li> <p>Critic Network: The critic network estimates the value of a given state by predicting the expected future reward. It is trained using temporal difference (TD) learning, minimizing the gap between the predicted reward and the actual reward observed from the environment.</p> </li> <li> <p>Advantage Function: The advantage function evaluates how much better a particular action is compared to the baseline action at a given state. It is computed as the difference between the expected reward for that action and the value of the state, helping the agent optimize its decision-making.</p> </li> <li> <p>Synchronized Updates: Unlike A3C, where multiple agents update asynchronously, A2C collects experiences from parallel environments and updates the actor and critic networks in a synchronized manner. This ensures more stable updates, as all experiences are used at once to compute the gradients and update the model.</p> </li> <li> <p>Exploration Strategy: A2C often uses exploration techniques like epsilon-greedy or entropy regularization to ensure that the agent explores various actions rather than always selecting the action with the highest expected reward.</p> </li> <li> <p>No Experience Replay: Like A3C, A2C does not rely on experience replay. Instead, it updates the network in real-time using data from multiple parallel environments, ensuring that the agent learns from recent and diverse experiences.</p> </li> </ul>"},{"location":"research/rl-algorithms/A2C/#rocket-league-use-case","title":"Rocket League Use Case","text":""},{"location":"research/rl-algorithms/A2C/#actor-network-in-rocket-league","title":"Actor Network in Rocket League","text":"<p>In Rocket League, the actor network in A2C learns to map complex game observations, such as the position, velocity, and orientation of the car and ball, to a set of continuous actions. These actions include turning, jumping, boosting, and throttle control. The challenge lies in the fact that Rocket League requires precise, fine-grained control over these continuous actions. The actor network must learn to balance various movement strategies, such as aerial shots, dribbling, and positioning for defense.</p>"},{"location":"research/rl-algorithms/A2C/#critic-network-in-rocket-league","title":"Critic Network in Rocket League","text":"<p>The critic network in Rocket League estimates the value of each game state based on observations. Rewards in Rocket League can be sparse, often derived from scoring goals or blocking opponents\u2019 shots. The critic network learns to estimate how valuable certain positions and movements are in relation to long-term objectives, like maintaining possession, controlling the ball, or preparing for an aerial maneuver. The critic's ability to predict long-term rewards helps the bot make more strategic decisions rather than short-term, reactionary moves.</p>"},{"location":"research/rl-algorithms/A2C/#advantage-function","title":"Advantage Function","text":"<p>In Rocket League, the advantage function is critical for determining how beneficial an action is relative to the baseline or average actions. For example, while aggressively challenging the ball might seem like a good action in the short term, the advantage function helps the bot assess whether retreating to a defensive position might provide a better long-term benefit. The advantage function helps the bot balance offensive and defensive strategies based on the current state of the game.</p>"},{"location":"research/rl-algorithms/A2C/#multiple-agents","title":"Multiple Agents","text":"<p>While A2C doesn\u2019t employ asynchronous updates like A3C, it still benefits from training with multiple environments running in parallel. In Rocket League, A2C can train bots by simulating multiple games (e.g., 1v1, 3v3) at once. The synchronized updates ensure that the actor and critic networks are updated with data from a diverse set of experiences, which can speed up learning and help the bot develop a broad set of skills, such as aerial control, positioning, and decision-making.</p> <p>This parallelism also enables faster exploration of different strategies, as multiple games offer a wider variety of experiences. However, unlike A3C, the updates are synchronized across all environments, which can lead to more stable learning but might slow down the exploration of extreme strategies.</p>"},{"location":"research/rl-algorithms/A2C/#key-challenges","title":"Key Challenges","text":"<ul> <li> <p>Sparse Rewards: Rocket League\u2019s rewards, primarily from scoring goals, are sparse and delayed, making it difficult for A2C to propagate useful feedback. The critic network may struggle to accurately evaluate the long-term impact of actions when rewards are delayed by several steps in a game.</p> </li> <li> <p>Exploration: The fast-paced, dynamic nature of Rocket League, along with its continuous action space, presents challenges for exploration. Good strategies, such as proper rotation, boost management, and ball control, emerge over time. A2C may struggle to adequately explore all these possibilities without additional exploration techniques.</p> </li> <li> <p>Team Coordination: In 3v3 matches, bots must work together to succeed. A2C, focusing on individual agent learning, might fail to capture team-based behaviors, such as coordinated passing or rotating between offensive and defensive roles. Each bot is learning its own policy, which can result in uncoordinated behaviors.</p> </li> <li> <p>Synchronized Learning: While synchronization improves stability, it might slow down exploration compared to A3C. In competitive, fast-paced environments like Rocket League, having asynchronous updates (as in A3C) could lead to more diverse strategies, whereas A2C\u2019s synchronized approach might favor convergence on safer, more average strategies.</p> </li> </ul>"},{"location":"research/rl-algorithms/A2C/#benefits-of-a2c-for-rocket-league","title":"Benefits of A2C for Rocket League","text":"<ol> <li> <p>Stable Learning: A2C\u2019s synchronized updates ensure more stable and consistent learning, especially in continuous action environments like Rocket League, where rapid changes in state can otherwise lead to erratic updates. This stability can be crucial for fine-tuning skills like aerial control or precise ball handling.</p> </li> <li> <p>Parallel Environments: By training across multiple environments simultaneously, A2C allows for faster learning and exploration of various strategies. In Rocket League, running multiple games in parallel helps the algorithm develop bots that can handle different game modes and play styles, such as 1v1 duels or full 3v3 team matches.</p> </li> <li> <p>No Experience Replay: Similar to A3C, A2C avoids experience replay, which is beneficial in Rocket League\u2019s real-time environment. The agent learns from the most recent data in multiple environments, ensuring that its learning process stays relevant to the current game dynamics.</p> </li> <li> <p>Better Advantage Estimation: The advantage function in A2C helps the agent evaluate short-term and long-term trade-offs, which is vital in a game like Rocket League, where success depends on both immediate actions (like hitting the ball) and long-term strategies (like positioning and defense).</p> </li> </ol>"},{"location":"research/rl-algorithms/A2C/#potential-improvements","title":"Potential Improvements","text":"<ol> <li> <p>Shaped Rewards: Introducing shaped rewards for intermediate objectives (such as ball possession, passing, or blocking) could help alleviate the sparse reward problem in Rocket League. Shaped rewards provide more immediate feedback and improve the learning speed by giving the agent more guidance.</p> </li> <li> <p>Coordination Mechanisms: Incorporating team coordination strategies, such as reward sharing or communication signals between agents, could enhance the performance of bots in 3v3 matches. This could lead to more cooperative behaviors, such as better rotation and passing between teammates.</p> </li> <li> <p>Entropy Regularization: To improve exploration, entropy regularization could be used to encourage more random actions, helping the agent discover diverse strategies. This is particularly useful in fast-paced games like Rocket League, where bots need to adapt to constantly changing game dynamics.</p> </li> <li> <p>Hierarchical Policies: Using hierarchical policies, where different bots or sub-policies focus on specialized tasks (e.g., offense, defense, or ball control), could help address the challenge of team coordination in Rocket League. This approach would allow for more strategic play and better role specialization.</p> </li> </ol>"},{"location":"research/rl-algorithms/A2C/#conclusion","title":"Conclusion","text":"<p>A2C offers a stable and synchronized approach to training agents in continuous, complex environments like Rocket League. While it may not explore as quickly as A3C due to its synchronized updates, A2C excels in delivering more consistent and stable learning. However, challenges like sparse rewards, team coordination, and exploration remain significant. With potential improvements like shaped rewards and hierarchical learning, A2C could become an effective algorithm for training sophisticated Rocket League bots capable of advanced gameplay strategies.</p>"},{"location":"research/rl-algorithms/A3C/","title":"A3C Algorithm","text":""},{"location":"research/rl-algorithms/A3C/#about","title":"About","text":"<p>A3C (Asynchronous Advantage Actor-Critic) is a reinforcement learning algorithm designed to train deep neural networks for decision-making in environments with large, continuous action spaces. It's widely applicable to various tasks due to its ability to efficiently handle such complexity. The core components of A3C include:</p> <ul> <li> <p>Actor Network: A deep neural network that maps observations to a probability distribution over possible actions. It is trained using policy gradient methods to maximize the expected future rewards.</p> </li> <li> <p>Critic Network: This network estimates the value of a given state by predicting the expected future reward. It is trained based on the temporal difference (TD) error, which is the gap between the predicted reward and the actual reward.</p> </li> <li> <p>Advantage Function: The advantage function evaluates how much better an action is compared to the average action in a given state. It is computed as the difference between the expected reward for a specific action and the baseline reward, which represents the expected reward of the current state.</p> </li> <li> <p>Asynchronous Updates: A3C enables multiple agents to run in parallel, each updating the actor and critic networks independently and asynchronously. This parallelism speeds up learning and enhances exploration across the state-action space.</p> </li> <li> <p>Exploration Strategy: To promote exploration, A3C often employs an epsilon-greedy strategy, where the agent occasionally selects random actions (with probability epsilon) instead of always choosing the highest-valued action (with probability 1-epsilon).</p> </li> <li> <p>Experience Replay: Unlike some other reinforcement learning algorithms, A3C doesn't use experience replay buffers. Instead, it relies on real-time updates from multiple agents running concurrently, which improves learning efficiency in continuous environments by leveraging recent experiences.</p> </li> </ul>"},{"location":"research/rl-algorithms/A3C/#rocket-league-usecase","title":"Rocket League usecase","text":""},{"location":"research/rl-algorithms/A3C/#actor-network-in-rocket-league","title":"Actor Network in Rocket League","text":"<p>In Rocket League, the actor network learns to map observations, such as the position and velocity of the car, ball, and other players, to a set of actions like steering, boosting, jumping, and rotating. Since Rocket League requires precise control, the actor network must handle continuous action values, such as the angle of turning or the throttle intensity. This makes the policy learning more intricate compared to games with discrete actions.</p>"},{"location":"research/rl-algorithms/A3C/#critic-network-in-rocket-league","title":"Critic Network in Rocket League","text":"<p>The critic network estimates the expected future reward based on the game state. Rewards in Rocket League could be designed around specific objectives, such as scoring a goal, preventing goals, maintaining possession, or controlling the ball effectively. The critic network learns the long-term impact of each state-action pair, helping the agent understand which actions will lead to better outcomes in the fast-changing environment.</p>"},{"location":"research/rl-algorithms/A3C/#advantage-function","title":"Advantage Function","text":"<p>In Rocket League, the advantage function helps determine how much better a particular action is compared to the baseline actions, such as simply chasing the ball or staying in a defensive position. Since Rocket League involves both short-term and long-term decision-making (e.g., immediate ball control vs. positioning for future plays), the advantage function is crucial in helping the bot make trade-offs between aggressive and defensive strategies.</p>"},{"location":"research/rl-algorithms/A3C/#multiple-agents","title":"Multiple Agents","text":"<p>A3C\u2019s ability to run multiple agents in parallel is a significant benefit when applied to Rocket League. Training multiple bots simultaneously across different games (such as 1v1, 3v3, or even 1v0 scenarios) helps the algorithm explore various strategies faster. The asynchronous nature of A3C means that each agent updates the shared actor and critic networks independently, which accelerates convergence and enables more diverse strategies to emerge.</p> <p>In Rocket League, bots must learn various skills, such as positioning, aerial maneuvers, and decision-making in both offense and defense, and asynchronous updates allow for richer exploration of this complex state-action space.</p>"},{"location":"research/rl-algorithms/A3C/#key-challenges","title":"Key Challenges","text":"<ul> <li>Sparse rewards: The primary reward signal in Rocket League is scoring goals, which can be sparse and delayed. This makes it harder for A3C to propagate useful feedback throughout an episode.</li> <li>Exploration challenges: In Rocket League, good strategies (e.g., positioning, boost management) emerge over time, but A3C might struggle with adequate exploration of these strategies due to the fast-paced environment.</li> <li>Team Coordination: In 3v3 matches, bots need to coordinate with teammates to succeed, which is particularly difficult for A3C to handle because of its focus on individual agent learning. Bots may fail to adopt team-oriented behaviors like passing, covering teammates' mistakes, or rotating properly (i.e., the defensive or offensive roles taken during gameplay).</li> <li>Multi-Agent Interference: In a competitive, multi-agent environment, the actions of other players (bots or humans) can lead to \u201cinterference,\u201d where one agent\u2019s learning process disrupts another\u2019s. This can degrade overall learning quality and result in inefficient or counterproductive strategies, such as two bots going for the ball simultaneously.</li> </ul>"},{"location":"research/rl-algorithms/A3C/#conclusion","title":"Conclusion","text":"<p>While A3C has strong potential for Rocket League bot training, particularly in simpler 1v1 scenarios, it faces significant challenges in multi-agent and team-based environments like 3v3. Modifications to A3C, such as incorporating memory or hierarchical learning, may be required to overcome these obstacles. Synchronization, sparse rewards, and handling complex states remain key hurdles to its effective use in Rocket League training.</p>"},{"location":"research/rl-algorithms/DQN/","title":"Deep Q Network","text":"<p>Theoretical Background:</p> <p>Q-learning is a basic reinforcement learning algorithm. It is model-free (meaning it doesn\u2019t need a model of the environment), off-policy (it learns regardless of the strategy we follow), and value-based. Q-learning uses a Q-table to store and update values for each state-action pair. These values are updated using the Bellman equation. What\u2019s interesting is that the Bellman equation doesn\u2019t rely on the previous state but instead uses the current state and the estimated value of the next state.</p> <p></p> <p>Image created by:Mike Wang</p> <p>A major issue with Q-learning is that the Q-table method becomes impractical when there are too many states and actions to calculate. In such cases, the table can grow too large.</p> <p>The solution to this problem is Deep Q-learning (DQL), which is an extension of Q-learning that uses a neural network instead of a Q-table. The difference is that in Q-learning, the Q-table gives us simple values, but in DQL, the neural network outputs action-value pairs.</p> <p></p> <p>Image created by:Sergios Karagiannakos</p> <p>Deep Q-Network (DQN) refers to the neural network used in DQL to approximate Q-values. This helps in handling environments with large or continuous state spaces, making decision-making more efficient.</p> <p>Implementation: Here you can read about the main steps to create basic DQN.</p> <p>DQN Step-by-Step algorithm </p> <ol> <li>Create Policy Neural Network:    Start by defining the policy network, which is a neural network responsible for approximating the Q-values for a given state-action pair. The network takes the current state as input and outputs a Q-value for each possible action in that state.  </li> <li>Create Target Network:    Copy the policy network to create the target network.  </li> <li>This is a copy of the policy network that is updated periodically to help stabilize the learning process. By using a fixed target network, we reduce the risk of overestimating Q-values, which can occur when both the predicted and target Q-values are updated simultaneously.  </li> <li>The target network is synchronized with the policy network after a fixed number of steps.  </li> <li>Action Selection with Epsilon-Greedy Exploration:    During training, the agent chooses actions using an epsilon-greedy strategy:  </li> <li>With probability epsilon (\ud835\udf16), the agent selects a random action.  </li> <li>With probability (1 \u2013 epsilon), the agent selects the action with the highest Q-value predicted by the policy network.  </li> <li>As training progresses, epsilon is gradually reduced to encourage more exploitation over time.  </li> <li>Store Experiences in Replay Buffer:    After every action, store the experience in a replay buffer (also called experience replay).  </li> <li>Each experience consists of a tuple (state, action, reward, next state, done), representing a single step from one state to another as a result of taking a specific action.  </li> <li>Replay Memory is a buffer that allows the agent to store and randomly sample mini-batches of experiences to break the correlation between consecutive transitions. It helps improve the stability and convergence of the learning process.  </li> <li>Put Information into the Policy Network:    Sample a mini-batch of experiences from the replay buffer and input the current state (s) into the policy network. The policy network will output the predicted Q-values for all actions in that state.  </li> <li>Obtain Q-value from Target Network:    For each experience in the mini-batch, pass the next state (s') through the target network to get the Q-value for the next state. Use these Q-values to compute the target Q-value.  </li> <li>Calculate the Target Q-value using the Bellman Equation    The target Q-value (ideal reward) is calculated using the Bellman equation:</li> </ol> <p>$$    Q_{\\text{target}}(s, a) = r + \\gamma \\max_{a'} Q_{\\text{target}}(s', a')    $$</p> <p>where:</p> <ul> <li>( r ) is the reward received after taking action ( a ) in state ( s ),</li> <li>( \\gamma ) is the discount factor that accounts for future rewards,</li> <li> <p>( \\max_{a'} Q_{\\text{target}}(s', a') ) is the maximum Q-value for the next state ( s' ) as predicted by the target network.</p> </li> <li> <p>Compute the Loss and Backpropagate    The loss is calculated as the mean squared error between the predicted Q-values from the policy network and the target Q-values from the target network:</p> </li> </ul> <p>$$    \\text{Loss} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( Q_{\\text{predicted}}(s_i, a_i) - Q_{\\text{target}}(s_i, a_i) \\right)^2    $$</p> <ul> <li> <p>Use this loss to perform backpropagation and update the weights of the policy network using stochastic gradient descent.</p> </li> <li> <p>Train the Policy Network:    The policy network is updated to minimize the difference between the predicted Q-values and the target Q-values, ensuring that it learns to make better action-value predictions over time.  </p> </li> <li>Repeat Steps 3-8:     Continue the process by repeating steps 3 through 8:  <ul> <li>Select actions using epsilon-greedy.  </li> <li>Store experiences in the replay buffer.  </li> <li>Sample mini-batches, calculate the loss, and update the policy network.  </li> </ul> </li> <li>Synchronize Policy and Target Networks:     After a fixed number of steps (usually after several episodes or steps), synchronize the target network with the policy network by copying the weights from the policy network to the target network. This keeps the target network up to date, but prevents it from changing too frequently.</li> </ul> <p>Recommend to watch this video: https://youtu.be/EUrWGTCGzlA?si=67Bblssy8OYbeTwJ</p> <p>Exemples of impementation: cleanrl/cleanrl/dqn_atari.py at master \u00b7 vwxyzjn/cleanrl (github.com) RLAlgorithms/DQN_From_Scratch_Atari.ipynb at main \u00b7 JayBaileyCS/RLAlgorithms (github.com)</p> <p>Problems and solutions:</p> <p>A common problem when using neural networks in reinforcement learning is maintaining stability during the training process. To address this issue, we can use Double DQN. This approach involves using two DQNs: one is a copy of the network, which is used to generate the predicted term, and the second one is the actual DQN used for training, responsible for generating the target term. This method helps to stabilize training and improves the accuracy of the Q-values, particularly during the early stages.</p> <p>Another common challenge is tuning the correct hyperparameters for the neural network. Some key parameters include:</p> <ul> <li>Epsilon value: It's better to start with a smaller epsilon to prevent the model from excessive exploration before the exploitation phase begins.  </li> <li>Learning rate: It should not be too large in order to slow down the learning process, which helps in stabilizing the training.  </li> <li>Memory size: The replay memory should be large enough to store sufficient experiences for effective learning.</li> </ul> <p>Here you can read how different parameters can be involved in results: Solving Lunar Lander with Double Dueling Deep Q-Network and PyTorch (drawar.github.io)</p> <p>Resources:</p> <p>First time using: 1312.5602 (arxiv.org) 1711.07478 (arxiv.org)</p> <p>Foundation about RL: The secrets behind Reinforcement Learning | by Sergios Karagiannakos | Towards Data Science</p> <p>Foundation about Q-learning: Q Learning and Deep Q Networks. The journey to Reinforcement learning\u2026 | by Sergios Karagiannakos | Towards Data Science</p> <p>Compera to Policy Gradient: Deep Q Network vs Policy Gradients - An Experiment on VizDoom with Keras | Felix Yu (flyyufelix.github.io)</p> <p>Useful repo: RLgym/DQN/DQNtest.py at master \u00b7 curefate/RLgym (github.com) RL-Gym-PyTorch/CartPole/DQN at main \u00b7 lexiconium/RL-Gym-PyTorch (github.com) deep-learning/reinforcement/Q-learning-cart.ipynb at master \u00b7 udacity/deep-learning (github.com) </p>"},{"location":"research/rl-algorithms/model-based-algorithms-for-rocket-league-bots-alphazero/","title":"Research and Documentation of Model-Based Algorithms for Rocket League Bots (e.g., AlphaZero)","text":""},{"location":"research/rl-algorithms/model-based-algorithms-for-rocket-league-bots-alphazero/#introduction-to-model-based-algorithms-and-alphazero","title":"Introduction to Model-Based Algorithms and AlphaZero","text":"<p>Model-Based Algorithms in artificial intelligence (AI) and machine learning are algorithms that incorporate a model of the environment in which they operate. This model allows the algorithm to simulate future states of the environment, making decisions based on predictions of these future states. Unlike model-free algorithms, which rely solely on past experiences and rewards to make decisions, model-based algorithms use a planning component that evaluates potential actions by simulating their outcomes.</p> <p>AlphaZero is a well-known example of a model-based algorithm that combines deep reinforcement learning and tree search methods (such as Monte Carlo Tree Search, MCTS). Originally developed by DeepMind for board games like chess, shogi, and Go, AlphaZero learns to play these games at a superhuman level without prior knowledge of the game rules, relying solely on self-play and learning from the results.</p> <p>Key Features of AlphaZero:</p> <ul> <li>Self-Play and Reinforcement Learning: The algorithm plays against itself to generate data, using reinforcement learning to improve its strategy.</li> <li>Monte Carlo Tree Search (MCTS): AlphaZero uses MCTS to explore possible future states of the game, making it a model-based algorithm.</li> <li>Deep Neural Networks: AlphaZero employs deep neural networks to evaluate board positions and predict moves, improving its efficiency and performance over time.</li> </ul>"},{"location":"research/rl-algorithms/model-based-algorithms-for-rocket-league-bots-alphazero/#use-of-model-based-algorithms-in-rocket-league-bots","title":"Use of Model-Based Algorithms in Rocket League Bots","text":""},{"location":"research/rl-algorithms/model-based-algorithms-for-rocket-league-bots-alphazero/#research-on-alphazero-like-algorithms-in-rocket-league","title":"Research on AlphaZero-like Algorithms in Rocket League","text":"<ol> <li> <p>Existing Applications:</p> </li> <li> <p>There is limited direct evidence of AlphaZero itself being applied to Rocket League. However, the fundamental principles of AlphaZero (self-play, reinforcement learning, MCTS) have influenced several AI research projects for Rocket League.</p> </li> <li> <p>Most Rocket League bots have traditionally relied on simpler rule-based systems or heuristic-based AI. However, recent advancements have seen more sophisticated applications of machine learning techniques, such as reinforcement learning.</p> </li> <li> <p>Useful Links:</p> </li> <li> <p>https://arxiv.org/abs/1712.01815 - link to a science paper about mastering chess and shogi using AlphaZero and AlphaZeroGo algorithms.</p> </li> <li>https://arxiv.org/abs/2006.16712 - link to a science paper with a survey about the usage of Model-Based Reinforcement Learning (various methods and algorithms).</li> <li>https://www.youtube.com/watch?v=wuSQpLinRB4 - a video tutorial about usage of AlphaZero algorithm in various games.</li> <li> <p>https://suragnair.github.io/posts/alphazero.html - a link to a simple Alpha(Go) Zero tutorial.</p> </li> <li> <p>Challenges and Limitations:</p> </li> <li> <p>Continuous and High-Dimensional Action Spaces: Rocket League\u2019s action space is continuous and high-dimensional, making it challenging for traditional AlphaZero-like approaches that were designed for discrete, lower-dimensional action spaces like board games.</p> </li> <li>Real-Time Decision-Making: AlphaZero relies heavily on MCTS, which requires substantial computational time for planning and decision-making. Adapting MCTS for real-time environments like Rocket League is challenging due to the need for rapid decision-making.</li> <li>Complex Physics and Environment: The game\u2019s physics engine introduces additional complexity, as accurate modeling requires understanding and predicting complex interactions between the ball, cars, and environment.</li> </ol>"},{"location":"research/rl-algorithms/model-based-algorithms-for-rocket-league-bots-alphazero/#3-performance-of-model-based-algorithms-in-rocket-league-applications","title":"3. Performance of Model-Based Algorithms in Rocket League Applications","text":"<p>As of now, model-based algorithms like AlphaZero have not been directly applied to Rocket League bots in publicly available research or commercial applications. However, there are several performance-related insights and possibilities:</p> <ul> <li> <p>Potential Performance Advantages:</p> </li> <li> <p>If adapted successfully, model-based algorithms could outperform current Rocket League bots by leveraging planning and prediction capabilities. This would allow bots to anticipate opponents\u2019 moves and strategize several steps ahead, similar to how AlphaZero plays board games.</p> </li> <li> <p>They could also learn to exploit the game's underlying physics and optimize movement and ball control in ways that model-free algorithms might not achieve.</p> </li> <li> <p>Empirical Performance:</p> </li> <li> <p>There is no empirical data available on AlphaZero-like performance in Rocket League due to the lack of direct implementations. However, similar high-level concepts (reinforcement learning and tree-based search) have shown promise in achieving competitive play against human players when computational constraints are managed effectively.</p> </li> </ul>"},{"location":"research/rl-algorithms/model-based-algorithms-for-rocket-league-bots-alphazero/#4-conclusion-and-future-directions","title":"4. Conclusion and Future Directions","text":"<p>While there has not been a direct implementation of AlphaZero or similar model-based algorithms for Rocket League bots, the principles underlying these algorithms offer promising avenues for future research. The complex and dynamic nature of Rocket League presents unique challenges that require adaptation of traditional model-based methods, especially for real-time environments. Future research could focus on hybrid approaches that combine model-free reinforcement learning with model-based planning to optimize both decision-making speed and accuracy.</p> <p>Overall, while AlphaZero-like algorithms have not been fully realized in the Rocket League AI community, they represent an exciting frontier for advancing AI capabilities in complex, real-time environments.</p>"},{"location":"research/rl-algorithms/police-optimization-documentation-summer/","title":"Research and Documentation of Algorithm Categories for Rocket League Bot","text":"<ul> <li>Police Optimization - Igor Malkovskiy</li> </ul> <p>Materials:</p> <p>Policy Gradient Algorithms Explained by Arthur Juliani | Towards Data Science</p> <p>Parts 2:Kinds of RL Algorithms</p> <p>Part 3:Intro to Police optimization</p> <p>Proximal Police Optimization</p> <p>What is Police Optimization?</p> <p>Policy Optimization is a method in reinforcement learning that focuses on improving the policy directly by optimizing its parameters. Unlike value-based methods like Q-Learning, which optimize the value function, Policy Optimization aims to improve the agent\u2019s behavior policy continuously. You can find exactly how Police Optimization works in the links above.</p> <p>Examples of uses:</p> <p>Building-a-reinforcement-learning-agent-that-can-play-rocket-leagu</p> <p>RL_Final_Report.pdf</p> <p>MA_Neville_Walo_Seer_RLRL.pdf - there is a github with this work attached to the second page from below</p> <p>Why Policy Optimization is/Isn't Popular for Developing Bots in Rocket League:</p> <p>Policy optimization is popular for developing bots in Rocket League due to its adaptability in dynamic environments. It allows for direct optimization of the agent\u2019s behavior, crucial in a fast-paced game where decision-making needs to be continuous and efficient. Methods like PPO (Proximal Policy Optimization) can adjust actions in real-time, enhancing performance in complex scenarios like ball control or aerial maneuvers.</p> <p>However, policy optimization isn't always favored because it often requires significant computational resources and data. Training can be slow, and models may struggle with long-term strategy, which is key in Rocket League. Thus, alternative methods like imitation learning or hybrid approaches might be preferred in certain contexts for efficiency and strategic depth.</p> <p>Conclusion:</p> <p>In conclusion, while policy optimization provides a powerful framework for developing adaptable and responsive bots in Rocket League, its computational intensity and challenges with long-term strategy can limit its widespread use. Depending on the specific goals\u2014whether it's real-time performance or strategic planning\u2014developers may choose alternative or hybrid approaches to better balance efficiency and effectiveness in bot behavior.</p>"},{"location":"research/visualizations/interface-overlay-research/","title":"Overlay Interface for Rocket League","text":""},{"location":"research/visualizations/interface-overlay-research/#summary","title":"Summary","text":"<p>After researching various libraries and tools for creating in-game overlays for Rocket League, the most viable solution appears to be using the built-in rendering functionality provided by the RLBot framework. The RLBot renderer offers a set of functions that allow for the creation of 2D and 3D graphics, text, and other visual elements within the game environment.</p>"},{"location":"research/visualizations/interface-overlay-research/#rendering-game-information","title":"Rendering Game Information","text":"<p>The RLBot renderer provides the following functions that can be used to display the state of the bot and other game-related information:</p> <ul> <li><code>renderer.create_color(r, g, b, a)</code></li> <li><code>renderer.draw_rect_2d(x, y, width, height, color)</code></li> <li><code>renderer.draw_line_2d(x1, y1, x2, y2, color)</code></li> <li><code>renderer.draw_string_2d(x, y, scale_x, scale_y, color, text)</code></li> <li><code>renderer.draw_rect_3d(x, y, z, width, height, depth, color)</code></li> <li><code>renderer.draw_string_3d(x, y, z, scale_x, scale_y, color, text)</code></li> <li><code>renderer.draw_line_3d(x1, y1, z1, x2, y2, z2, color)</code></li> <li><code>renderer.draw_line_2d_3d(x1, y1, z1, x2, y2, z2, color)</code></li> </ul> <p>These functions can be used to create a custom overlay within the Rocket League game environment, displaying various information such as the bot's status, game scores, and other relevant data.</p>"},{"location":"research/visualizations/interface-overlay-research/#links","title":"Links","text":"<ul> <li>RLBot rendering Wiki</li> <li>Python bot rendering example lines 59-62</li> </ul>"},{"location":"research/visualizations/interface-overlay-research/#clickable-overlays","title":"Clickable Overlays","text":"<p>Creating a fully interactive, clickable overlay within Rocket League would be a more complex task, as there is no dedicated solution for this functionality. Some potential approaches include using Python libraries like OpenCV, AutoGUI, IMGUI, or Tkinter. However, the development effort required for a clickable overlay may not be worth the benefits, as the built-in rendering capabilities of RLBot can provide a sufficient level of visualization and information display.</p>"},{"location":"research/visualizations/interface-overlay-research/#fps-meter","title":"FPS Meter","text":"<p>For monitoring the game's frame rate (FPS), the most straightforward solution is to use the built-in FPS counter provided by Rocket League. This can be enabled by going to the game's settings, navigating to the \"Interface\" tab, and enabling the \"Performance Graphs\" option, or use F10 hotkey. This will display the current FPS within the game window. Alternative could be to use Widows Game Bar.</p>"},{"location":"research/visualizations/training-visualization-reaserch/","title":"Research on Data Rendering Options During Training Rocket League Bot","text":""},{"location":"research/visualizations/training-visualization-reaserch/#introduction","title":"Introduction","text":"<p>When training a Rocket League bot, visualizing the data in real-time can be challenging. The <code>rlgym-ppo</code> library does not provide a straightforward way to display data in real-time using external game visualizers. The best method for real-time visualization is to run the trained bot using <code>rl-bot</code>.</p>"},{"location":"research/visualizations/training-visualization-reaserch/#real-time-visualization-with-rl-bot","title":"Real-Time Visualization with rl-bot","text":"<p><code>rl-bot</code> allows for real-time visualization by running the trained bot directly in the Rocket League game. This method provides an immediate and interactive way to observe the bot's performance and behavior during training. Methodes are described in <code>interface-overlay-research.md</code>.</p>"},{"location":"research/visualizations/training-visualization-reaserch/#integration-with-weights-biases-wandb","title":"Integration with Weights &amp; Biases (wandb)","text":"<p><code>rlgym-ppo</code> offers excellent integration with Weights &amp; Biases (wandb). Wandb is a tool that helps track machine learning experiments, visualize metrics, and collaborate with team members. It provides a comprehensive dashboard to monitor various aspects of the training process, such as loss, accuracy, and other performance metrics.</p>"},{"location":"research/visualizations/training-visualization-reaserch/#data-displayed-during-ppo-training","title":"Data Displayed During PPO Training","text":"<p>During Proximal Policy Optimization (PPO) training, the following types of data are typically displayed:</p> <ul> <li>Training Loss: Indicates how well the model is learning.</li> <li>Reward: Measures the performance of the bot.</li> <li>Episode Length: Shows the duration of each training episode.</li> <li>Policy and Value Function Metrics: Provide insights into the policy and value function updates.</li> </ul> <p>These metrics can be effectively displayed using wandb, which offers real-time tracking and visualization capabilities. However, for real-time game visualization, <code>rl-bot</code> is the preferred tool.</p>"},{"location":"research/visualizations/training-visualization-reaserch/#summary","title":"Summary","text":"<p>In conclusion, for real-time visualizations of the Rocket League bot's performance, <code>rl-bot</code> should be used. For monitoring and visualizing machine learning-related data during and after training, <code>wandb</code> provides a robust solution. Combining both tools allows for comprehensive analysis and visualization of the training process.</p>"}]}